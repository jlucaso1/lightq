This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    test.yml
example/
  index.ts
scripts/
  build.ts
src/
  classes/
    job.ts
    queue.ts
    scheduler.ts
    worker.ts
  interfaces/
    index.ts
  macros/
    loadLuaScript.ts
    luamin.d.ts
  scripts/
    lua/
      addJob.lua
      extendLock.lua
      moveDelayedToWait.lua
      moveToActive.lua
      moveToCompleted.lua
      moveToFailed.lua
      retryJob.lua
    lua.ts
  utils/
    index.ts
  index.ts
test/
  smq.test.ts
.gitignore
LICENSE
package.json
README.md
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/classes/scheduler.ts">
import { EventEmitter } from "node:events";
import type {
  JobSchedulerOptions,
  JobTemplate,
  RedisClient,
  SchedulerData,
  SchedulerRepeatOptions,
} from "../interfaces";
import type { Queue } from "./queue";
import { createRedisClient } from "../utils";
import { Cron } from "croner";

// Helper to generate scheduler-specific keys
function getSchedulerKeys(prefix: string, queueName: string) {
  const base = `${prefix}:${queueName}:schedulers`;
  return {
    base,
    hashPrefix: `${base}:`, // Prefix for individual scheduler hashes HASH (`base:<schedulerId>`)
    index: `${base}:index`, // ZSET (score: nextRun, member: schedulerId)
  };
}

type SchedulerKeys = ReturnType<typeof getSchedulerKeys>;

export class JobScheduler extends EventEmitter {
  private queue: Queue<any, any, any>;
  private client: RedisClient;
  private opts: Required<Omit<JobSchedulerOptions, "connection">>; // Make internal opts required
  private keys: SchedulerKeys;
  private checkTimer: NodeJS.Timeout | null = null;
  private running: boolean = false;
  private closing: Promise<void> | null = null;

  // Cache for parsed cron expressions to avoid re-parsing
  private cronCache = new Map<string, Cron>();

  constructor(queue: Queue<any, any, any>, opts: JobSchedulerOptions) {
    super();
    this.queue = queue;

    // Use the queue's client if available and compatible, otherwise create new
    // This assumes queue instance is passed and has a client.
    // If scheduler can be standalone, client creation needs adjustment.
    this.client = queue.client || createRedisClient(opts.connection);

    this.opts = {
      prefix: opts.prefix ?? "lightq",
      schedulerPrefix: opts.schedulerPrefix ?? opts.prefix ?? "lightq",
      checkInterval: opts.checkInterval ?? 5000, // Check every 5 seconds by default
      defaultJobOptions: opts.defaultJobOptions ?? {},
    };

    this.keys = getSchedulerKeys(this.opts.schedulerPrefix, queue.name);

    this.client.on(
      "error",
      (err) => this.emit("error", `Redis Error: ${err.message}`),
    );
  }

  /**
   * Creates or updates a job scheduler.
   * @param schedulerId A unique identifier for this scheduler.
   * @param repeat The repeat options (cron pattern or interval).
   * @param template The template for jobs to be generated.
   */
  async upsertJobScheduler<TData = any, TName extends string = string>(
    schedulerId: string,
    repeat: SchedulerRepeatOptions,
    template: JobTemplate<TData, TName>,
  ): Promise<void> {
    if (this.closing) throw new Error("Scheduler is closing");
    if (!schedulerId) throw new Error("Scheduler ID cannot be empty");

    const now = Date.now();
    let type: "cron" | "every";
    let value: string | number;
    let tz: string | undefined;
    let nextRun: number | undefined;

    // Validate and parse repeat options
    if (repeat.pattern) {
      if (typeof repeat.pattern !== "string") {
        throw new Error("Invalid cron pattern");
      }
      type = "cron";
      value = repeat.pattern;
      tz = repeat.tz;
      try {
        const interval = this.parseCron(value, now, tz);
        nextRun = interval.nextRun()?.getTime();
      } catch (err: any) {
        throw new Error(`Invalid cron pattern: ${err.message}`);
      }
    } else if (repeat.every) {
      if (typeof repeat.every !== "number" || repeat.every <= 0) {
        throw new Error(
          "Invalid 'every' value: must be a positive number (milliseconds)",
        );
      }
      type = "every";
      value = repeat.every;
      // First run is 'every' ms from now
      nextRun = now + value;
    } else {
      throw new Error(
        "Invalid repeat options: must provide 'pattern' or 'every'",
      );
    }

    if (!nextRun) {
      console.warn(
        `Scheduler ${schedulerId} has no nextRun calculated. Skipping upsert.`,
      );
      return;
    }

    // Prepare data for Redis Hash
    const schedulerKey = `${this.keys.hashPrefix}${schedulerId}`;
    const schedulerData: Omit<SchedulerData, "lastRun"> = {
      id: schedulerId,
      type,
      value,
      tz,
      nextRun,
      name: template.name,
      data: template.data, // Store as provided, will be stringified below
      opts: template.opts, // Store as provided, will be stringified below
    };

    try {
      const multi = this.client.multi();
      // Use HMSET/HSET compatible with older Redis versions if needed
      // Modern ioredis handles object mapping for HSET
      multi.hset(schedulerKey, {
        ...schedulerData,
        data: JSON.stringify(schedulerData.data ?? {}), // Ensure data is stored as JSON string
        opts: JSON.stringify(schedulerData.opts ?? {}), // Ensure opts are stored as JSON string
        nextRun: nextRun.toString(), // Store numbers as strings
        value: value.toString(), // Store value as string
      });

      multi.zadd(this.keys.index, nextRun, schedulerId);
      await multi.exec();
      this.emit("upsert", schedulerId, schedulerData);

      // Potentially wake up the poller if the new nextRun is sooner than the current check interval
      // For simplicity, we rely on the regular checkInterval for now.
    } catch (err: any) {
      this.emit(
        "error",
        `Failed to upsert scheduler ${schedulerId}: ${err.message}`,
      );
      throw err; // Re-throw
    }
  }

  /**
   * Removes a job scheduler.
   * @param schedulerId The ID of the scheduler to remove.
   */
  async removeJobScheduler(schedulerId: string): Promise<boolean> {
    if (this.closing) {
      console.warn("Cannot remove scheduler, scheduler is closing.");
      return false; // Or throw error
    }
    if (!schedulerId) return false;

    const schedulerKey = `${this.keys.hashPrefix}${schedulerId}`;

    try {
      const multi = this.client.multi();
      multi.del(schedulerKey);
      multi.zrem(this.keys.index, schedulerId);
      const results = await multi.exec();

      // Check results: [[null, delCount], [null, zremCount]]
      const deleted = !!results && results[0]?.[1] === 1;
      if (deleted) {
        this.emit("remove", schedulerId);
      }
      return deleted;
    } catch (err: any) {
      this.emit(
        "error",
        `Failed to remove scheduler ${schedulerId}: ${err.message}`,
      );
      throw err; // Re-throw
    }
  }

  /** Starts the background polling process */
  start(): void {
    if (this.running || this.closing) {
      return;
    }
    this.running = true;
    this.emit("start");
    this._scheduleNextCheck();
    console.log(
      `JobScheduler for queue "${this.queue.name}" started. Checking every ${this.opts.checkInterval}ms.`,
    );
  }

  /** Stops the background polling process */
  stop(): void {
    if (!this.running || this.closing) {
      return;
    }
    this.running = false;
    if (this.checkTimer) {
      clearTimeout(this.checkTimer);
      this.checkTimer = null;
    }
    this.emit("stop");
    console.log(`JobScheduler for queue "${this.queue.name}" stopped.`);
  }

  /** Closes the scheduler and its Redis connection (if owned) */
  async close(): Promise<void> {
    if (this.closing) {
      return this.closing;
    }

    this.closing = (async () => {
      this.emit("closing");
      this.stop(); // Stop polling

      // Note: This implementation assumes the client might be shared with the Queue.
      // If the scheduler owns its client, it should close it here.
      // await this.client.quit();

      // Clear cron cache
      this.cronCache.clear();

      this.emit("closed");
      console.log(`JobScheduler for queue "${this.queue.name}" closed.`);
    })();

    return this.closing;
  }

  private _scheduleNextCheck(): void {
    if (!this.running || this.closing) return;

    this.checkTimer = setTimeout(() => {
      // Run check and then schedule the next one
      this._checkAndProcessDueJobs()
        .catch((err) => {
          this.emit("error", `Error during scheduler check: ${err.message}`);
        })
        .finally(() => {
          this._scheduleNextCheck(); // Always reschedule after completion or error
        });
    }, this.opts.checkInterval);
  }

  private async _checkAndProcessDueJobs(): Promise<void> {
    if (!this.running || this.closing) return;

    const now = Date.now();
    const limit = 50; // Process up to 50 due schedulers per check cycle

    try {
      // Find schedulers due to run (nextRun <= now)
      const results = await this.client.zrangebyscore(
        this.keys.index,
        "-inf", // Min score
        now, // Max score (current time)
        "LIMIT",
        "0",
        limit.toString(),
      );

      if (results.length === 0) {
        return; // Nothing to do
      }

      // --- Simple processing loop (potential for race conditions in distributed env) ---
      // A Lua script would be needed for true atomicity if multiple schedulers run
      for (const schedulerId of results) {
        if (!this.running || this.closing) break; // Stop if scheduler stopped mid-loop
        await this._processSingleScheduler(schedulerId, now);
      }
      // --- End Simple processing loop ---
    } catch (err: any) {
      // Handle Redis errors during the check
      if (err.message !== "Connection is closed.") {
        this.emit("error", `Scheduler check failed: ${err.message}`);
      }
    }
  }

  // Processes a single scheduler identified by ID
  private async _processSingleScheduler(
    schedulerId: string,
    now: number,
  ): Promise<void> {
    const schedulerKey = `${this.keys.hashPrefix}${schedulerId}`;
    let schedulerData: Record<string, string> | null = null;
    let parsedData: SchedulerData | null = null;
    let newNextRun: number | null = null;

    try {
      // 1. Get Scheduler Data
      schedulerData = await this.client.hgetall(schedulerKey);
      if (!schedulerData || Object.keys(schedulerData).length === 0) {
        // Scheduler was likely removed between ZRANGE and HGETALL
        await this.client.zrem(this.keys.index, schedulerId); // Clean up index just in case
        console.warn(
          `Scheduler data for ${schedulerId} not found, removing from index.`,
        );
        return;
      }

      // Basic check to prevent processing too early if checkInterval is very short
      const currentNextRun = parseInt(schedulerData.nextRun || "0", 10);
      if (currentNextRun > now) {
        // This scheduler isn't actually due yet (e.g., updated by another process)
        // Ensure ZSET score is correct, then skip
        await this.client.zadd(
          this.keys.index,
          "NX",
          currentNextRun,
          schedulerId,
        ); // Update score if it doesn't exist
        return;
      }

      // Parse the data from Redis strings
      parsedData = this.parseSchedulerData(schedulerData);
      if (!parsedData) {
        throw new Error("Failed to parse scheduler data from Redis.");
      }

      // 2. Calculate Next Run Time BEFORE adding the job
      const lastRun = now; // Use current time as the effective last run
      if (parsedData.type === "cron") {
        try {
          const interval = this.parseCron(
            parsedData.value as string,
            lastRun,
            parsedData.tz,
          );
          newNextRun = interval.nextRun()?.getTime() || null;
        } catch (err: any) {
          throw new Error(
            `Invalid cron pattern (${parsedData.value}): ${err.message}`,
          );
        }
      } else {
        // type === 'every'
        newNextRun = lastRun + (parsedData.value as number);
      }

      if (!newNextRun) {
        console.warn(
          `Scheduler ${schedulerId} has no new nextRun calculated. Skipping job addition.`,
        );
        return;
      }

      // 3. Add Job to Queue
      const jobName = parsedData.name;
      const jobData = parsedData.data;
      // Combine default opts with template opts, then with queue defaults
      const jobOpts = {
        ...this.queue.opts.defaultJobOptions, // Queue defaults
        ...this.opts.defaultJobOptions, // Scheduler defaults (if any)
        ...parsedData.opts, // Template specific opts
        // Force override crucial options
        jobId: undefined, // Scheduled jobs cannot have custom IDs managed externally
        delay: undefined, // Job is added to run NOW, not delayed further
      };

      // Add the job (fire and forget for the scheduler's perspective, worker handles it)
      // Important: No 'await' here if we want to update scheduler state quickly,
      // but awaiting ensures the job was accepted by Redis before updating the schedule.
      // Let's await for robustness.
      const addedJob = await this.queue.add(jobName, jobData, jobOpts);
      this.emit("job_added", schedulerId, addedJob);

      // 4. Update Scheduler State in Redis (Hash and ZSet)
      const multi = this.client.multi();
      multi.hset(
        schedulerKey,
        "nextRun",
        newNextRun.toString(),
        "lastRun",
        lastRun.toString(),
      );
      multi.zadd(this.keys.index, newNextRun, schedulerId); // Update score in ZSET
      await multi.exec();
    } catch (err: any) {
      this.emit(
        "error",
        `Failed to process scheduler ${schedulerId}: ${err.message}`,
      );
      // Decide how to handle failures:
      // - Remove the scheduler?
      // - Mark it as failed?
      // - Retry later? (Requires more complex state)
      // For now, we log the error. The scheduler might be picked up again if its
      // nextRun time doesn't get updated, potentially causing duplicate jobs if the
      // job add succeeded but the state update failed. Atomicity (Lua) is key here.
      // If nextRun *was* calculated, try to update just that to avoid tight loop.
      if (newNextRun && parsedData) {
        try {
          const recoveryNextRun = Math.max(
            newNextRun,
            now + this.opts.checkInterval,
          ); // Push it at least one interval out
          await this.client
            .multi()
            .hset(schedulerKey, "nextRun", recoveryNextRun.toString())
            .zadd(this.keys.index, recoveryNextRun, schedulerId)
            .exec();
          console.warn(
            `Scheduler ${schedulerId} processing error, pushed nextRun forward.`,
          );
        } catch (recoveryErr: any) {
          console.error(
            `Failed recovery update for scheduler ${schedulerId}: ${recoveryErr.message}`,
          );
          // Consider removing from index if recovery fails to prevent infinite loop
          await this.client.zrem(this.keys.index, schedulerId);
          console.error(
            `Removed scheduler ${schedulerId} from index due to recovery failure.`,
          );
        }
      }
    }
  }

  // Helper to parse scheduler data stored as strings in Redis hash
  private parseSchedulerData(
    hashData: Record<string, string>,
  ): SchedulerData | null {
    try {
      const data: Partial<SchedulerData> = {};
      data.id = hashData.id;
      data.type = hashData.type as "cron" | "every";
      data.tz = hashData.tz;
      data.nextRun = parseInt(hashData.nextRun || "0", 10);
      data.lastRun = hashData.lastRun
        ? parseInt(hashData.lastRun, 10)
        : undefined;
      data.name = hashData.name;
      data.data = JSON.parse(hashData.data || "{}");
      data.opts = JSON.parse(hashData.opts || "{}");

      if (data.type === "cron") {
        data.value = hashData.value;
      } else if (data.type === "every") {
        data.value = parseInt(hashData.value || "0", 10);
      } else {
        return null; // Invalid type
      }

      if (
        !data.id ||
        !data.type ||
        !data.value ||
        !data.name ||
        isNaN(data.nextRun)
      ) {
        return null; // Missing essential fields
      }

      return data as SchedulerData;
    } catch (e) {
      this.emit("error", `Error parsing scheduler data from Redis: ${e}`);
      return null;
    }
  }

  // Helper to parse cron expression, using cache
  private parseCron(
    pattern: string,
    currentDate: number | Date,
    tz?: string,
  ): Cron {
    const cacheKey = `${pattern}_${tz || "local"}`;
    if (this.cronCache.has(cacheKey)) {
      const expr = this.cronCache.get(cacheKey)!;
      // Important: Reset the iterator state for the cached expression
      // expr.reset(new Date(currentDate));
      return expr;
    }

    const interval = new Cron(pattern, { timezone: tz });
    this.cronCache.set(cacheKey, interval); // Cache it

    return interval;
  }
}
</file>

<file path="scripts/build.ts">
await Bun.build({
  entrypoints: ["./src/index.ts"],
  outdir: "./dist",
  target: "node",
  external: ["croner", "ioredis"],
});
export {};
</file>

<file path="src/classes/job.ts">
import type { JobData, JobOptions } from "../interfaces";
import type { QueueKeys } from "../utils";
import type { Queue } from "./queue";

export class Job<TData = any, TResult = any, TName extends string = string> {
  id: string;
  name: TName;
  data: TData;
  opts: JobOptions;
  timestamp: number;
  delay: number;
  attemptsMade: number;
  processedOn?: number;
  finishedOn?: number;
  returnValue?: TResult;
  failedReason?: string;
  stacktrace?: string[];
  lockedUntil?: number;
  lockToken?: string;

  private queue: Queue;
  private queueKeys: QueueKeys;

  constructor(queue: Queue, jobData: JobData<TData>) {
    this.queue = queue;
    this.queueKeys = queue.keys;

    this.id = jobData.id;
    this.name = jobData.name as TName;
    this.data = jobData.data;
    this.opts = jobData.opts;
    this.timestamp = jobData.timestamp;
    this.delay = jobData.delay;
    this.attemptsMade = jobData.attemptsMade;
    this.processedOn = jobData.processedOn;
    this.finishedOn = jobData.finishedOn;
    this.returnValue = jobData.returnValue;
    this.failedReason = jobData.failedReason;
    this.stacktrace = jobData.stacktrace;
    this.lockedUntil = jobData.lockedUntil;
    this.lockToken = jobData.lockToken;
  }

  async updateProgress(progress: number | object): Promise<void> {
    // NOTE: Progress updates are complex with atomicity.
    // For simplicity, maybe store progress directly in the job hash?
    // This isn't ideal for real-time updates but simplifies the core.
    // Or implement a specific script like BullMQ.
    console.warn("updateProgress not fully implemented in this simple version");
    const jobKey = `${this.queueKeys.jobs}:${this.id}`;
    const currentData = (await this.queue.client.hgetall(jobKey)) as any; // TODO: Improve typing
    if (currentData) {
      currentData.progress = JSON.stringify(progress);
      await this.queue.client.hset(
        jobKey,
        "progress",
        JSON.stringify(progress),
      );
      // Maybe emit local event?
    } else {
      throw new Error(`Job ${this.id} not found for progress update.`);
    }
  }

  toData(): JobData<TData> {
    return {
      id: this.id,
      name: this.name,
      data: this.data,
      opts: this.opts,
      timestamp: this.timestamp,
      delay: this.delay,
      attemptsMade: this.attemptsMade,
      processedOn: this.processedOn,
      finishedOn: this.finishedOn,
      returnValue: this.returnValue,
      failedReason: this.failedReason,
      stacktrace: this.stacktrace,
      lockedUntil: this.lockedUntil,
      lockToken: this.lockToken,
    };
  }

  static fromData<TData = any, TResult = any, TName extends string = string>(
    queue: Queue,
    jobData: JobData<TData>,
  ): Job<TData, TResult, TName> {
    return new Job<TData, TResult, TName>(queue, jobData);
  }

  static fromRedisHash<
    TData = any,
    TResult = any,
    TName extends string = string,
  >(
    queue: Queue,
    hashData: Record<string, string>,
  ): Job<TData, TResult, TName> {
    const jobData: Partial<JobData<TData>> = {};
    jobData.id = hashData.id;
    jobData.name = hashData.name;
    jobData.data = JSON.parse(hashData.data || "{}");
    jobData.opts = JSON.parse(hashData.opts || "{}");
    jobData.timestamp = parseInt(hashData.timestamp || "0", 10);
    jobData.delay = parseInt(hashData.delay || "0", 10);
    jobData.attemptsMade = parseInt(hashData.attemptsMade || "0", 10);
    jobData.processedOn = hashData.processedOn
      ? parseInt(hashData.processedOn, 10)
      : undefined;
    jobData.finishedOn = hashData.finishedOn
      ? parseInt(hashData.finishedOn, 10)
      : undefined;
    jobData.returnValue = hashData.returnValue
      ? JSON.parse(hashData.returnValue)
      : undefined;
    jobData.failedReason = hashData.failedReason;
    jobData.stacktrace = hashData.stacktrace
      ? JSON.parse(hashData.stacktrace)
      : [];
    jobData.lockedUntil = hashData.lockedUntil
      ? parseInt(hashData.lockedUntil, 10)
      : undefined;
    jobData.lockToken = hashData.lockToken;

    // TODO: Add progress parsing if implemented

    return new Job<TData, TResult, TName>(queue, jobData as JobData<TData>);
  }
}
</file>

<file path="src/interfaces/index.ts">
import { Cluster, Redis, type RedisOptions } from "ioredis";

export type RedisClient = Redis | Cluster;

export interface QueueOptions {
  connection: RedisOptions | RedisClient;
  prefix?: string;
  defaultJobOptions?: JobOptions;
}

export interface WorkerOptions extends QueueOptions {
  concurrency?: number;
  lockDuration?: number;
  lockRenewTime?: number;
  removeOnComplete?: boolean | number;
  removeOnFail?: boolean | number;
}

export interface JobOptions {
  jobId?: string;
  delay?: number;
  attempts?: number;
  backoff?: number | { type: "fixed" | "exponential"; delay: number };
  removeOnComplete?: boolean | number;
  removeOnFail?: boolean | number;
}

export interface RedisJobOptions extends JobOptions {}

export interface JobData<T = any> {
  id: string;
  name: string;
  data: T;
  opts: RedisJobOptions;
  timestamp: number;
  delay: number;
  attemptsMade: number;
  // State tracking fields managed internally
  processedOn?: number;
  finishedOn?: number;
  returnValue?: any;
  failedReason?: string;
  stacktrace?: string[];
  // Lock info
  lockedUntil?: number;
  lockToken?: string;
}

export type SchedulerRepeatOptions =
  | {
    /** Cron pattern (e.g., '0 * * * *') */
    pattern: string;
    /** Optional timezone for cron pattern */
    tz?: string;
    /** Not used with pattern */
    every?: never;
  }
  | {
    /** Repeat interval in milliseconds */
    every: number;
    /** Not used with every */
    pattern?: never;
    tz?: never;
  };

/** Template for jobs created by a scheduler */
export interface JobTemplate<TData = any, TName extends string = string> {
  name: TName;
  data?: TData;
  opts?: Omit<JobOptions, "jobId" | "delay">; // Cannot set jobId or delay on scheduled jobs
}

/** Internal representation of scheduler data stored in Redis */
export interface SchedulerData extends JobTemplate {
  id: string;
  type: "cron" | "every";
  value: string | number; // Cron pattern or 'every' ms
  tz?: string;
  nextRun: number; // Timestamp (ms)
  lastRun?: number; // Timestamp (ms)
  // lockUntil?: number; // Potential future addition for distributed locking
}

/** Options for the JobScheduler instance */
export interface JobSchedulerOptions extends QueueOptions {
  // Reuse QueueOptions for connection
  /** How often the scheduler checks for due jobs (milliseconds) */
  checkInterval?: number;
  /** Prefix for scheduler-specific keys */
  schedulerPrefix?: string;
}
</file>

<file path="src/macros/luamin.d.ts">
declare module "luamin" {
  export function minify(lua: string): string;
}
</file>

<file path="src/scripts/lua/addJob.lua">
--[[
  Adds a job to the wait or delayed list and creates its data hash.

  Input:
    KEYS[1] jobsPrefix (e.g., lightq:myqueue:jobs)
    KEYS[2] waitKey    (e.g., lightq:myqueue:wait)
    KEYS[3] delayedKey (e.g., lightq:myqueue:delayed)

    ARGV[1] jobId
    ARGV[2] name
    ARGV[3] dataJson
    ARGV[4] optsJson
    ARGV[5] timestamp (creation time as string)
    ARGV[6] delayMs   (initial delay as string)
    ARGV[7] attemptsMade (as string, should be "0" initially)

  Output:
    jobId if successful
    0 if job already exists
]]
local jobsPrefix = KEYS[1]
local waitKey = KEYS[2]
local delayedKey = KEYS[3]

local jobId = ARGV[1]
local name = ARGV[2]
local dataJson = ARGV[3]
local optsJson = ARGV[4]
local timestamp_str = ARGV[5]
local delayMs_str = ARGV[6]
local attemptsMade_str = ARGV[7] -- Keep as string

local timestamp = tonumber(timestamp_str) -- Convert for calculation
local delayMs = tonumber(delayMs_str)     -- Convert for calculation and check

local jobKey = jobsPrefix .. ':' .. jobId

if redis.call("EXISTS", jobKey) == 1 then
  return 0 -- Job already exists
end

-- Create the job hash with initial data
-- Pass numeric values as strings explicitly
redis.call("HMSET", jobKey,
  "id", jobId,
  "name", name,
  "data", dataJson,
  "opts", optsJson,
  "timestamp", timestamp_str, -- Use original string
  "delay", delayMs_str,     -- Use original string
  "attemptsMade", attemptsMade_str -- Use original string
  -- Note: initial progress, returnvalue, failedReason, stacktrace, locks are omitted
)

if delayMs > 0 then
  local delayedTimestamp = timestamp + delayMs
  -- Pass score as string
  redis.call("ZADD", delayedKey, tostring(delayedTimestamp), jobId)
  -- TODO: Add marker for delayed jobs if implementing efficient blocking
else
  redis.call("LPUSH", waitKey, jobId) -- Add to head for FIFO processing with RPOPLPUSH
  -- TODO: Add marker for waiting jobs if implementing efficient blocking
end

return jobId
</file>

<file path="src/scripts/lua/extendLock.lua">
--[[
  Extends the lock duration for an active job if the token matches.

  Input:
    KEYS[1] jobsPrefix

    ARGV[1] jobId
    ARGV[2] lockToken
    ARGV[3] lockDuration (milliseconds)
    ARGV[4] now (timestamp ms)

  Output:
    newLockedUntil timestamp (ms) if successful
    0 if lock mismatch or job does not exist
]]
local jobsPrefix = KEYS[1]

local jobId = ARGV[1]
local lockToken = ARGV[2]
local lockDuration = tonumber(ARGV[3])
local now = tonumber(ARGV[4])

local jobKey = jobsPrefix .. ':' .. jobId

local currentLockToken = redis.call("HGET", jobKey, "lockToken")

-- Check if job exists and token matches
if currentLockToken and currentLockToken == lockToken then
  local newLockedUntil = now + lockDuration
  redis.call("HSET", jobKey, "lockedUntil", newLockedUntil)
  return newLockedUntil
else
  -- Lock mismatch, job doesn't exist, or lock field missing
  return 0
end
</file>

<file path="src/scripts/lua/moveDelayedToWait.lua">
--[[
  Moves jobs from the delayed set to the wait list if their time has come.

  Input:
    KEYS[1] delayedKey
    KEYS[2] waitKey

    ARGV[1] now (timestamp ms as string)
    ARGV[2] limit (max jobs to move per call as string)

  Output:
    Number of jobs moved
]]
local delayedKey = KEYS[1]
local waitKey = KEYS[2]

local now_str = ARGV[1]
local limit_str = ARGV[2]
-- No need to convert limit to number unless used for complex looping logic later
-- local limit = tonumber(limit_str)

-- Get jobs ready to be moved (score <= now)
-- Pass score/limit arguments as strings
local jobIds = redis.call("ZRANGEBYSCORE", delayedKey, "-inf", now_str, "LIMIT", "0", limit_str) -- Use "-inf" for min, pass args as strings

local numJobIds = #jobIds
if numJobIds > 0 then
  -- Remove them from the delayed set (jobId is already a string)
  for i = 1, numJobIds do
      local jobId = jobIds[i]
      redis.call("ZREM", delayedKey, jobId) -- jobId from ZRANGEBYSCORE is string
  end

  -- Add them to the wait list (jobId is already a string)
  for i = 1, numJobIds do
      local jobId = jobIds[i]
      redis.call("LPUSH", waitKey, jobId) -- jobId from ZRANGEBYSCORE is string
  end

  -- TODO: Update delay field in job hashes to 0? (Optional optimization)
  -- TODO: Emit 'waiting' events via Pub/Sub?
  -- TODO: Add wait marker?
end

return numJobIds -- Return the count
</file>

<file path="src/scripts/lua/moveToActive.lua">
--[[
  Atomically moves a job from wait to active, locks it, and returns job data.

  Input:
    KEYS[1] waitKey
    KEYS[2] activeKey
    KEYS[3] jobsPrefix

    ARGV[1] lockToken (unique token for this worker+job)
    ARGV[2] lockDuration (milliseconds)
    ARGV[3] now (current timestamp ms)

  Output:
    {jobId, jobDataArray} if successful (jobDataArray is flat array from HGETALL)
    nil if no job is available in the wait queue
]]
local waitKey = KEYS[1]
local activeKey = KEYS[2]
local jobsPrefix = KEYS[3]

local lockToken = ARGV[1]
local lockDuration = tonumber(ARGV[2])
local now = tonumber(ARGV[3])

-- RPOP from wait, LPUSH to active (maintains FIFO within active for easier check)
local jobId = redis.call("RPOPLPUSH", waitKey, activeKey)

if jobId then
  local jobKey = jobsPrefix .. ':' .. jobId
  local lockedUntil = now + lockDuration

  -- Check if job hash exists (safety check)
  if redis.call("EXISTS", jobKey) == 0 then
      -- Job data missing, remove from active and return nil
      redis.call("LREM", activeKey, -1, jobId) -- Remove the instance we just pushed
      return nil
  end

  -- Set lock info and processing time directly in the job hash
  redis.call("HMSET", jobKey,
    "lockToken", lockToken,
    "lockedUntil", lockedUntil,
    "processedOn", now
    -- Increment attemptsMade only when moving to failed/completed later?
    -- Or maybe increment 'attemptsStarted' here? Simpler to do later.
  )

  -- Return jobId and all job data
  local jobData = redis.call("HGETALL", jobKey)
  return {jobId, jobData}
end

return nil
</file>

<file path="src/scripts/lua/moveToCompleted.lua">
--[[
  Moves a job from active to completed, releases lock, stores result.

  Input:
    KEYS[1] activeKey
    KEYS[2] completedKey (ZSET, score=timestamp)
    KEYS[3] jobsPrefix

    ARGV[1] jobId
    ARGV[2] returnValueJson
    ARGV[3] removeOptionString ('true', 'false', or max count number as string)
    ARGV[4] now (timestamp ms)
    ARGV[5] lockToken

  Output:
     0: Success
    -1: Lock mismatch or job hash missing
    -2: Job not found in active list
]]
local activeKey = KEYS[1]
local completedKey = KEYS[2]
local jobsPrefix = KEYS[3]

local jobId = ARGV[1]
local returnValueJson = ARGV[2]
local removeOption = ARGV[3] -- Keep as string for comparison
local now = tonumber(ARGV[4])
local lockToken = ARGV[5]

local jobKey = jobsPrefix .. ':' .. jobId

-- 1. Verify Lock
local currentLockToken = redis.call("HGET", jobKey, "lockToken")
if not currentLockToken or currentLockToken ~= lockToken then
  return -1 -- Lock mismatch or job missing
end

-- 2. Remove from Active list
-- LREM count: 0=all, >0=from head, <0=from tail. Use -1 to remove one from tail (matches RPOPLPUSH)
local removedCount = redis.call("LREM", activeKey, -1, jobId)
if removedCount == 0 then
  return -2 -- Job not in active list (or already moved)
end

-- 3. Handle removal or move to Completed set
if removeOption == 'true' then
  -- Remove job data completely
  redis.call("DEL", jobKey)
else
  -- Update job data: add result, finish time, remove lock info
  redis.call("HMSET", jobKey,
    "returnValue", returnValueJson,
    "finishedOn", now
  )
  redis.call("HDEL", jobKey, "lockToken", "lockedUntil", "processedOn")

  -- Add to completed set, score is finish timestamp
  redis.call("ZADD", completedKey, now, jobId)

  -- Optional: Trim completed set (implement if needed)
  local keepCount = tonumber(removeOption)

  if keepCount and keepCount > 0 then
    redis.call("ZREMRANGEBYRANK", completedKey, 0, -(keepCount + 1))
  end
end

-- TODO: Emit 'completed' event via Pub/Sub if needed

return 0 -- Success
</file>

<file path="src/scripts/lua/moveToFailed.lua">
--[[
  Moves a job from active to failed, releases lock, stores reason/stack.

  Input:
    KEYS[1] activeKey
    KEYS[2] failedKey (ZSET, score=timestamp)
    KEYS[3] jobsPrefix

    ARGV[1] jobId
    ARGV[2] failedReason
    ARGV[3] stacktraceJson
    ARGV[4] removeOptionString ('true', 'false', or max count number as string)
    ARGV[5] now (timestamp ms)
    ARGV[6] lockToken
    ARGV[7] finalAttemptsMade (the count *after* the last attempt)

  Output:
     0: Success
    -1: Lock mismatch or job hash missing
    -2: Job not found in active list
]]
local activeKey = KEYS[1]
local failedKey = KEYS[2]
local jobsPrefix = KEYS[3]

local jobId = ARGV[1]
local failedReason = ARGV[2]
local stacktraceJson = ARGV[3]
local removeOption = ARGV[4]
local now = tonumber(ARGV[5])
local lockToken = ARGV[6]
local attemptsMade = tonumber(ARGV[7])


local jobKey = jobsPrefix .. ':' .. jobId

-- 1. Verify Lock
local currentLockToken = redis.call("HGET", jobKey, "lockToken")
if not currentLockToken or currentLockToken ~= lockToken then
  return -1 -- Lock mismatch or job missing
end

-- 2. Remove from Active list
local removedCount = redis.call("LREM", activeKey, -1, jobId)
if removedCount == 0 then
  return -2 -- Job not in active list
end

-- 3. Handle removal or move to Failed set
if removeOption == 'true' then
  -- Remove job data completely
  redis.call("DEL", jobKey)
else
  -- Update job data: add failure info, finish time, remove lock info
  redis.call("HMSET", jobKey,
    "failedReason", failedReason,
    "stacktrace", stacktraceJson,
    "finishedOn", now,
    "attemptsMade", attemptsMade
    -- Keep attemptsMade
  )
  redis.call("HDEL", jobKey, "lockToken", "lockedUntil", "processedOn")

  -- Add to failed set, score is finish timestamp
  redis.call("ZADD", failedKey, now, jobId)

  -- Optional: Trim failed set (implement if needed)
  local keepCount = tonumber(removeOption)
  if keepCount and keepCount > 0 then
    redis.call("ZREMRANGEBYRANK", failedKey, 0, -(keepCount + 1))
  end
end

-- TODO: Emit 'failed' event via Pub/Sub if needed

return 0 -- Success
</file>

<file path="src/scripts/lua/retryJob.lua">
--[[
  Moves a job from active list back to wait or delayed (for retry/backoff).
  Cleans up lock and state fields from the job hash.

  Input:
    KEYS[1] activeKey
    KEYS[2] delayedKey
    KEYS[3] waitKey
    KEYS[4] jobsPrefix

    ARGV[1] jobId
    ARGV[2] delayMs (backoff delay)
    ARGV[3] now (timestamp ms)
    ARGV[4] newFailedReason (optional: update reason on retry?)
    ARGV[5] newStacktraceJson (optional: update stacktrace on retry?)

  Output:
     0: Success
    -1: Job not found in failed set
    -2: Job hash data missing
    -3: Job not found in active list
]]
local activeKey = KEYS[1]
local delayedKey = KEYS[2]
local waitKey = KEYS[3]
local jobsPrefix = KEYS[4]

local jobId = ARGV[1]
local delayMs = tonumber(ARGV[2])
local now = tonumber(ARGV[3])
local newFailedReason = ARGV[4] -- Potentially store retry reason? Keep original for now.
local newStacktraceJson = ARGV[5] -- Potentially store retry stack? Keep original for now.

local jobKey = jobsPrefix .. ':' .. jobId

-- 2. Check if job data exists
if redis.call("EXISTS", jobKey) == 0 then
    return -2 -- Job data is missing, cannot retry
end

-- 1. Remove from Active list
-- Use count -1 to remove only the last matching element (consistent with RPOPLPUSH)
local removedCount = redis.call("LREM", activeKey, -1, jobId)
if removedCount == 0 then
    return -3 -- Job not found in active list
end

-- 3. Update Job Data: Increment attempts, clear finish/fail state
redis.call("HINCRBY", jobKey, "attemptsMade", 1)
redis.call("HDEL", jobKey, "finishedOn", "failedReason", "stacktrace", "processedOn", "lockToken", "lockedUntil")
-- Optionally update reason/stacktrace if ARGV[4]/[5] are provided

-- 4. Move to Wait or Delayed
if delayMs > 0 then
  local delayedTimestamp = now + delayMs
  redis.call("ZADD", delayedKey, delayedTimestamp, jobId)
  -- TODO: Add delayed marker if needed
else
  redis.call("LPUSH", waitKey, jobId) -- Add to head for FIFO processing
  -- TODO: Add wait marker if needed
end

-- TODO: Emit 'retrying' or 'waiting' event via Pub/Sub if needed

return 0 -- Success
</file>

<file path="src/utils/index.ts">
import IORedis, { type RedisOptions } from "ioredis";
import type { RedisClient } from "../interfaces";

export function createRedisClient(
  connection: RedisOptions | RedisClient,
): RedisClient {
  if (connection instanceof IORedis || connection instanceof IORedis.Cluster) {
    // TODO: Consider implications of sharing/duplicating connections
    return connection;
  }
  return new IORedis(connection as RedisOptions);
}

export function getQueueKeys(prefix: string, queueName: string) {
  const base = `${prefix}:${queueName}`;
  return {
    base,
    wait: `${base}:wait`, // LIST - New jobs
    active: `${base}:active`, // LIST - Processing jobs
    delayed: `${base}:delayed`, // ZSET - Jobs scheduled for later
    completed: `${base}:completed`, // ZSET - Successfully finished jobs
    failed: `${base}:failed`, // ZSET - Jobs that exhausted retries
    locks: `${base}:locks`, // HASH or SET - JobId -> LockToken/Timestamp
    jobs: `${base}:jobs`, // HASH - JobId -> Job Data
    meta: `${base}:meta`, // HASH - Queue metadata (paused flag, etc.)
    // Potentially others: events (simple pub/sub?), priority (if added)
  };
}

export type QueueKeys = ReturnType<typeof getQueueKeys>;

export function delay(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}
</file>

<file path="src/index.ts">
export * from "./classes/queue";
export * from "./classes/worker";
export * from "./classes/job";
export * from "./classes/scheduler";
export * from "./interfaces";
</file>

<file path="LICENSE">
MIT License

Copyright (c) [year] [fullname]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path=".github/workflows/test.yml">
name: Run Tests & Coverage

on:
  push:
    branches: [main]
    paths: # Run on push only if these files/folders change
      - "src/**"
      - "test/**"
      - "package.json"
      - "bun.lockb"
      - "tsconfig.json"
      - "scripts/build.ts"
      - ".github/workflows/test.yml"

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      # Docker service for Redis, needed by the tests
      redis:
        image: redis:latest
        ports:
          - 6379:6379 # Map container port 6379 to host port 6379
        options: >- # Health check to ensure Redis is ready before tests start
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest # Or specify a specific Bun version

      - name: Install dependencies
        run: bun install --frozen-lockfile # Use frozen lockfile for reproducibility

      - name: Run tests with coverage
        run: bun test --coverage --coverage-reporter=lcov --coverage-dir=./coverage
        env:
          # Make Redis service available to tests
          REDIS_HOST: localhost
          REDIS_PORT: 6379 # Corresponds to the host port mapped in 'services'

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }} # Optional: Store Codecov token in GitHub secrets for private repos
          files: ./coverage/lcov.info # Path to the generated lcov file
          fail_ci_if_error: true # Optional: Fail the build if Codecov upload fails
          verbose: true # Optional: Enable verbose logs
</file>

<file path="src/classes/queue.ts">
import { EventEmitter } from "node:events";
import type {
  JobData,
  JobOptions,
  JobSchedulerOptions,
  JobTemplate,
  QueueOptions,
  RedisClient,
  SchedulerRepeatOptions,
} from "../interfaces";
import { createRedisClient, getQueueKeys, type QueueKeys } from "../utils";
import { LuaScripts } from "../scripts/lua";
import { randomUUID } from "node:crypto";
import { Job } from "./job";
import { Pipeline } from "ioredis";
import { Buffer } from "node:buffer";
import { JobScheduler } from "./scheduler";

export class Queue<
  TData = any,
  TResult = any,
  TName extends string = string,
> extends EventEmitter {
  readonly name: string;
  readonly client: RedisClient;
  readonly opts: QueueOptions;
  readonly keys: QueueKeys;
  private scripts: LuaScripts;
  private closing: Promise<void> | null = null;
  private prefix: string;
  private scheduler: JobScheduler | null = null;
  private schedulerOpts: JobSchedulerOptions | null = null;

  constructor(name: string, opts: QueueOptions) {
    super();
    this.name = name;
    this.opts = opts;
    this.prefix = opts.prefix ?? "lightq";
    this.client = createRedisClient(opts.connection);
    this.keys = getQueueKeys(this.prefix, this.name);
    this.scripts = new LuaScripts(this.client);

    this.schedulerOpts = {
      connection: this.client, // Share connection by default
      prefix: this.prefix,
      defaultJobOptions: this.opts.defaultJobOptions,
      // Make scheduler prefix configurable if needed, defaults to queue prefix
      schedulerPrefix: opts.prefix ?? "lightq",
      // Allow overriding checkInterval via QueueOptions potentially?
      // checkInterval: (opts as any).schedulerCheckInterval ?? 5000,
    };

    this.client.on("error", (err) => this.emit("error", err));
    this.client.on("ready", () => this.emit("ready"));
  }

  async add(
    name: TName,
    data: TData,
    opts?: JobOptions,
  ): Promise<Job<TData, TResult, TName>> {
    if (this.closing) {
      throw new Error("Queue is closing");
    }

    const mergedOpts: JobOptions = {
      attempts: 1,
      delay: 0,
      ...this.opts.defaultJobOptions,
      ...opts,
    };

    if (mergedOpts?.jobId?.startsWith("scheduler:")) {
      console.warn(
        `Attempting to manually add a job with a reserved scheduler ID prefix: ${mergedOpts.jobId}. Schedulers manage their own job IDs.`,
      );
      // Decide whether to throw an error or just remove the conflicting ID
      // mergedOpts.jobId = undefined; // Option: Silently fix
      throw new Error(
        `Cannot manually add job with reserved scheduler ID: ${mergedOpts.jobId}`,
      );
    }

    const jobId = mergedOpts.jobId ?? randomUUID();
    const timestamp = Date.now();

    const jobData: JobData<TData> = {
      id: jobId,
      name,
      data,
      opts: mergedOpts,
      timestamp,
      delay: mergedOpts.delay ?? 0,
      attemptsMade: 0,
    };

    await this.scripts.addJob(this.keys, jobData);

    const job = Job.fromData<TData, TResult, TName>(this, jobData);
    this.emit("waiting", job);
    return job;
  }

  async addBulk(
    jobs: { name: TName; data: TData; opts?: JobOptions }[],
  ): Promise<Job<TData, TResult, TName>[]> {
    if (this.closing) {
      throw new Error("Queue is closing");
    }

    const jobInstances: Job<TData, TResult, TName>[] = [];
    const pipeline = this.client.pipeline();
    if (!(pipeline instanceof Pipeline)) {
      throw new Error("Pipeline is not an instance of Pipeline");
    }
    const timestamp = Date.now();

    for (const jobDef of jobs) {
      const mergedOpts: JobOptions = {
        attempts: 1,
        delay: 0,
        ...this.opts.defaultJobOptions,
        ...jobDef.opts,
      };
      if (mergedOpts?.jobId?.startsWith("scheduler:")) {
        console.warn(
          `Attempting to bulk add a job with a reserved scheduler ID prefix: ${mergedOpts.jobId}. Skipping this specific job.`,
        );
        continue;
      }
      const jobId = mergedOpts.jobId ?? randomUUID();

      const jobData: JobData<TData> = {
        id: jobId,
        name: jobDef.name,
        data: jobDef.data,
        opts: mergedOpts,
        timestamp,
        delay: mergedOpts.delay ?? 0,
        attemptsMade: 0,
      };

      this.scripts.addJob(this.keys, jobData, pipeline);
      jobInstances.push(Job.fromData<TData, TResult, TName>(this, jobData));
    }

    await pipeline.exec();

    jobInstances.forEach((job) => this.emit("waiting", job));
    return jobInstances;
  }

  async getJob(jobId: string): Promise<Job<TData, TResult, TName> | null> {
    const jobKey = `${this.keys.jobs}:${jobId}`;
    const data = await this.client.hgetall(jobKey);
    if (Object.keys(data).length === 0) {
      return null;
    }
    return Job.fromRedisHash<TData, TResult, TName>(this, data);
  }

  async getJobCounts(): Promise<{
    wait: number;
    active: number;
    delayed: number;
    completed: number;
    failed: number;
  }> {
    const keys = this.keys;
    const multi = this.client.multi();
    multi.llen(keys.wait);
    multi.llen(keys.active);
    multi.zcard(keys.delayed);
    multi.zcard(keys.completed);
    multi.zcard(keys.failed);

    const results = await multi.exec();
    const counts = results?.map((res) => res[1] as number) ?? [0, 0, 0, 0, 0];

    return {
      wait: counts[0]!,
      active: counts[1]!,
      delayed: counts[2]!,
      completed: counts[3]!,
      failed: counts[4]!,
    };
  }

  async close(): Promise<void> {
    if (!this.closing) {
      this.closing = (async () => {
        // --- Close scheduler first ---
        if (this.scheduler) {
          await this.scheduler.close();
        }
        // --- End Add ---
        try {
          // Now close the queue's client (potentially shared)
          // Check if client is still connected before quitting
          if (
            this.client &&
            ["connect", "ready"].includes(this.client.status)
          ) {
            await this.client.quit();
          } else {
            // If not connected or already closing/closed, just disconnect locally
            this.client?.disconnect();
          }
        } catch (err) {
          if ((err as Error).message !== "Connection is closed.") {
            console.error("Error during Redis quit:", err);
            this.client.disconnect(); // Force disconnect on error
          }
        }
      })();
    }
    return this.closing;
  }

  async executeScript(
    scriptName: string,
    keys: string[],
    args: (string | number | Buffer)[],
    pipeline?: Pipeline,
  ): Promise<any> {
    const command = pipeline || this.client;

    return (command as any)[scriptName]?.(...keys, ...args);
  }

  private getScheduler(): JobScheduler {
    if (!this.scheduler) {
      if (!this.schedulerOpts) {
        // This case shouldn't happen with the constructor logic
        throw new Error("Scheduler options not initialized.");
      }
      this.scheduler = new JobScheduler(this, this.schedulerOpts);
      // Forward scheduler events if needed
      this.scheduler.on("error", (err) => this.emit("scheduler_error", err));
      this.scheduler.on(
        "job_added",
        (schedulerId, job) =>
          this.emit("scheduler_job_added", schedulerId, job),
      );
      // ... forward other events like upsert, remove, start, stop ...
      this.scheduler.start(); // Start the scheduler when first accessed
    }
    return this.scheduler;
  }

  /**
   * Creates or updates a job scheduler associated with this queue.
   * Starts the scheduler polling process if it wasn't running.
   * @param schedulerId A unique identifier for this scheduler.
   * @param repeat The repeat options (cron pattern or interval).
   * @param template The template for jobs to be generated.
   */
  async upsertJobScheduler<TJobData = any, TJobName extends string = string>(
    schedulerId: string,
    repeat: SchedulerRepeatOptions,
    template: JobTemplate<TJobData, TJobName>,
  ): Promise<void> {
    return this.getScheduler().upsertJobScheduler(
      schedulerId,
      repeat,
      template,
    );
  }

  /**
   * Removes a job scheduler associated with this queue.
   * @param schedulerId The ID of the scheduler to remove.
   */
  async removeJobScheduler(schedulerId: string): Promise<boolean> {
    // Only try to remove if the scheduler has potentially been initialized
    if (this.scheduler) {
      return this.scheduler.removeJobScheduler(schedulerId);
    }
    // If scheduler never started, try removing directly via a temporary instance or static method (less ideal)
    // For simplicity, assume remove is called after scheduler might have been used.
    // Or, instantiate it just to remove (might start polling unnecessarily):
    // return this.getScheduler().removeJobScheduler(schedulerId);
    // Best: If scheduler is not active, deletion via direct redis command is okay if needed
    console.warn(
      "Attempted to remove scheduler before scheduler process was started for this queue.",
    );
    return false; // Indicate not removed as scheduler wasn't active
  }
}
</file>

<file path="src/macros/loadLuaScript.ts">
import * as path from "node:path";
import * as fs from "node:fs";
import luamin from "luamin";

/**
 * @internal
 * Macro function executed at bundle time to load Lua script content.
 * @param scriptName The base name of the Lua script (e.g., "addJob").
 * @returns The content of the Lua script file as a string.
 */
export function loadLuaScriptContent(scriptName: string): string {
  const scriptPath = path.join(
    import.meta.dirname,
    `../scripts/lua/${scriptName}.lua`,
  );
  console.log(
    `[Macro] Loading Lua script '${scriptName}' from path: ${scriptPath}`,
  );

  try {
    // Read the file synchronously to get the content
    const content = fs.readFileSync(scriptPath, "utf-8");
    // Return the content of the Lua script
    return luamin.minify(content);
  } catch (err) {
    console.error(
      `[Macro Error] Failed to load Lua script '${scriptName}' from path: ${scriptPath}`,
      err,
    );
    // Throwing an error here will cause the bun build process to fail.
    throw new Error(`[Macro Error] Could not load Lua script: ${scriptName}`);
  }
}
</file>

<file path=".gitignore">
# dependencies (bun install)
node_modules

# output
out
dist
*.tgz
dist

# code coverage
coverage
*.lcov

# logs
logs
_.log
report.[0-9]_.[0-9]_.[0-9]_.[0-9]_.json

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# caches
.eslintcache
.cache
*.tsbuildinfo

# IntelliJ based IDEs
.idea

# Finder (MacOS) folder config
.DS_Store
</file>

<file path="example/index.ts">
import { Job, Queue, Worker } from "../src"; // Use '@jlucaso/lightq' if installed as a package
import process from "node:process";

// --- 1. Configure Redis Connection ---
const redisConnectionOpts = {
  host: process.env.REDIS_HOST || "localhost",
  port: process.env.REDIS_PORT ? parseInt(process.env.REDIS_PORT, 10) : 6379,
  maxRetriesPerRequest: null, // Avoid default ioredis retry noise in demo
};

const QUEUE_NAME = "demo-tasks";

// --- 2. Define Job Data Interfaces (Best Practice) ---
interface EmailJobData {
  to: string;
  subject: string;
  body: string;
}
interface EmailJobResult {
  status: "success" | "failed";
  messageId?: string;
  error?: string;
}

interface ReportJobData {
  type: "daily" | "weekly";
  recipient: string;
}
interface ReportJobResult {
  status: "success" | "failed";
  reportUrl?: string;
  error?: string;
}

// --- 3. Create a Queue Instance ---
console.log(`[Queue] Creating queue "${QUEUE_NAME}"...`);
// Use a Union Type for the queue to handle multiple job types
const tasksQueue = new Queue<
  EmailJobData | ReportJobData,
  EmailJobResult | ReportJobResult
>(QUEUE_NAME, {
  connection: { ...redisConnectionOpts }, // Use a copy for potential modification needs
  // Default options applied to ALL jobs unless overridden
  defaultJobOptions: {
    attempts: 2,
    backoff: { type: "fixed", delay: 500 }, // Modest backoff for demo
    removeOnComplete: 50, // Keep details of last 50 completed jobs in the 'completed' set
    removeOnFail: 100, // Keep details of last 100 failed jobs in the 'failed' set
  },
});

tasksQueue.on(
  "ready",
  () => console.log("[Queue] Connected to Redis and ready."),
);
tasksQueue.on(
  "error",
  (error) => console.error("[Queue] Error:", error.message),
);

// --- 4. Define Functions to Add Different Job Types ---

async function addImmediateEmail() {
  console.log("[Queue] Adding immediate welcome email job...");
  const job = await tasksQueue.add("email:send-welcome", {
    to: "new-user@example.com",
    subject: "Welcome to LightQ!",
    body: "We're glad you're here.",
  });
  console.log(`[Queue] Added Job ID: ${job.id} (email:send-welcome)`);
}

async function addDelayedEmail() {
  const delayMs = 5000; // 5 seconds
  console.log(
    `[Queue] Adding delayed promo email job (Delay: ${delayMs}ms)...`,
  );
  const job = await tasksQueue.add("email:send-promo", {
    to: "subscriber@example.com",
    subject: "Special Offer Just For You!",
    body: "Check out these amazing deals.",
  }, {
    delay: delayMs, // Job-specific option
    attempts: 1, // Override default attempts
  });
  console.log(`[Queue] Added Job ID: ${job.id} (email:send-promo)`);
}

async function addFailingEmail() {
  console.log("[Queue] Adding email job designed to fail...");
  const job = await tasksQueue.add("email:send-invalid", {
    to: "fail@example.com", // Worker logic will cause this to fail
    subject: "This Should Fail",
    body: "Testing failure and retries.",
  }, {
    // Uses default attempts (2) and backoff (500ms fixed) from queue options
    removeOnFail: 5, // Keep only the last 5 records for this specific failing type
  });
  console.log(`[Queue] Added Job ID: ${job.id} (email:send-invalid)`);
}

// --- 5. Define and Add a Scheduled Job ---

async function setupScheduledReport() {
  const schedulerId = "report:weekly-summary";
  const repeatOptions = {
    // pattern: '0 0 * * MON', // Every Monday at midnight
    every: 10000, // For demo: Run every 10 seconds
    // tz: 'America/New_York' // Optional timezone for cron
  };
  const jobTemplate = {
    name: "report:generate-summary", // Job name for instances created by scheduler
    data: { type: "weekly", recipient: "admin@example.com" },
    opts: {
      // Specific options for generated report jobs
      removeOnComplete: 5, // Keep only 5 successful report job records
      attempts: 1,
    },
  };

  console.log(
    `[Scheduler] Upserting job scheduler "${schedulerId}" (runs every ${
      repeatOptions.every / 1000
    }s)...`,
  );
  try {
    await tasksQueue.upsertJobScheduler(
      schedulerId,
      repeatOptions,
      jobTemplate,
    );
    console.log(
      `[Scheduler] Scheduler "${schedulerId}" upserted successfully.`,
    );
  } catch (error) {
    console.error(
      `[Scheduler] Failed to upsert scheduler "${schedulerId}":`,
      error,
    );
  }

  // Optional: Demonstrate removing the scheduler after some time
  const REMOVAL_DELAY = 35 * 1000; // 35 seconds
  console.log(
    `[Scheduler] Will attempt to remove scheduler "${schedulerId}" in ${
      REMOVAL_DELAY / 1000
    } seconds.`,
  );
  setTimeout(async () => {
    console.log(
      `[Scheduler] Attempting to remove scheduler "${schedulerId}"...`,
    );
    try {
      const removed = await tasksQueue.removeJobScheduler(schedulerId);
      console.log(
        `[Scheduler] Scheduler "${schedulerId}" removal status:`,
        removed,
      );
    } catch (err) {
      console.error(
        `[Scheduler] Error removing scheduler "${schedulerId}":`,
        err,
      );
    }
  }, REMOVAL_DELAY);
}

tasksQueue.on("scheduler_error", (err) => {
  console.error("[Scheduler] Error:", err);
});
tasksQueue.on("scheduler_job_added", (schedulerId, job) => {
  console.log(
    `[Scheduler] Scheduler [${schedulerId}] added job: ${job.id} (${job.name})`,
  );
});

// --- 6. Create a Worker to Process Jobs ---

console.log(`[Worker] Creating worker for queue "${QUEUE_NAME}"...`);
// Use a Union Type for the worker matching the queue
const taskWorker = new Worker<
  EmailJobData | ReportJobData,
  EmailJobResult | ReportJobResult
>(
  QUEUE_NAME,
  // --- Processor Function ---
  async (
    job: Job<EmailJobData | ReportJobData, EmailJobResult | ReportJobResult>,
  ) => {
    console.log(
      `[Worker] Processing job ${job.id} (Name: ${job.name}, Attempt: ${
        job.attemptsMade + 1
      }/${job.opts.attempts ?? tasksQueue.opts.defaultJobOptions?.attempts})`,
    );

    // Handle different job types based on name
    if (job.name.startsWith("email:")) {
      const data = job.data as EmailJobData; // Type assertion
      console.log(
        `   -> Sending email to: ${data.to}, Subject: ${data.subject}`,
      );

      await delay(1000); // Simulate network latency

      if (data.to === "fail@example.com") {
        throw new Error(`Simulated email failure for ${data.to}`);
      }

      console.log(`   -> Email sent successfully for job ${job.id}`);
      return { status: "success", messageId: `fake-id-${job.id}` }; // Return EmailJobResult
    } else if (job.name.startsWith("report:")) {
      const data = job.data as ReportJobData; // Type assertion
      console.log(
        `   -> Generating ${data.type} report for: ${data.recipient}`,
      );

      await delay(1500); // Simulate report generation time

      console.log(`   -> Report generated successfully for job ${job.id}`);
      return {
        status: "success",
        reportUrl: `/reports/summary-${Date.now()}.pdf`,
      }; // Return ReportJobResult
    } else {
      console.warn(`[Worker] Unknown job name: ${job.name}`);
      throw new Error(`Unknown job name: ${job.name}`);
    }
  },
  // --- Worker Options ---
  {
    connection: { ...redisConnectionOpts }, // Worker needs its own connection options
    concurrency: 3, // Process up to 3 jobs concurrently
    lockDuration: 10000, // Lock jobs for 10 seconds while processing
    // removeOnComplete/Fail defaults can also be set here, overriding Queue defaults
    // removeOnComplete: 100,
  },
);

// --- 7. Listen to Worker Events (Optional but Recommended) ---

taskWorker.on("completed", (job, result) => {
  console.log(`[Worker] Job ${job.id} (${job.name}) COMPLETED successfully.`);
  // console.log(`   -> Result:`, result);
});

taskWorker.on("failed", (job, error) => {
  // Note: Job object might be undefined if the error occurred before a job was fully fetched/locked
  if (job) {
    console.error(
      `[Worker] Job ${job.id} (${job.name}) FAILED after ${job.attemptsMade} attempts.`,
    );
    console.error(`   -> Error: ${error.message}`);
  } else {
    console.error(
      `[Worker] A job failed with error, but job details are unavailable. Error: ${error.message}`,
    );
  }
});

taskWorker.on("error", (error, job) => {
  // Errors not related to a specific job's processing (e.g., connection, lock renewal)
  if (job) {
    console.error(`[Worker] Error related to job ${job.id}: ${error.message}`);
  } else {
    console.error(`[Worker] Generic worker error: ${error.message}`);
  }
});

taskWorker.on("active", (job) => {
  // console.log(`[Worker] Job ${job.id} (${job.name}) is now ACTIVE.`);
});

taskWorker.on("retrying", (job, error) => {
  console.warn(
    `[Worker] Job ${job.id} (${job.name}) failed, will RETRY. Error: ${error.message}`,
  );
});

taskWorker.on("ready", () => {
  console.log("[Worker] Worker is ready and connected to Redis.");
});

taskWorker.on("closing", () => {
  console.log("[Worker] Worker is closing...");
});

taskWorker.on("closed", () => {
  console.log("[Worker] Worker has closed.");
});

// --- 8. Start Adding Jobs and Running the Demo ---

async function runDemo() {
  console.log("--- Adding initial jobs ---");
  await addImmediateEmail();
  await addDelayedEmail();
  await addFailingEmail();

  console.log("--- Setting up scheduled jobs ---");
  await setupScheduledReport(); // Set up the recurring job

  console.log(
    "--- Worker started, processing jobs... (Press Ctrl+C to exit) ---",
  );
  // Worker starts processing automatically upon creation if not paused
}

runDemo().catch((error) => {
  console.error("Demo initialization failed:", error);
  process.exit(1);
});

// --- 9. Graceful Shutdown Handling ---

let isShuttingDown = false;
async function shutdown() {
  if (isShuttingDown) return;
  isShuttingDown = true;
  console.log("\n--- Initiating graceful shutdown ---");

  console.log("[Main] Closing worker (waiting for active jobs)...");
  await taskWorker.close(); // Wait for active jobs to finish

  console.log("[Main] Closing queue connection...");
  await tasksQueue.close(); // Close queue & scheduler connection

  console.log("--- Shutdown complete ---");
  process.exit(0);
}

process.on("SIGINT", shutdown); // Handle Ctrl+C
process.on("SIGTERM", shutdown); // Handle kill commands

function delay(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}
</file>

<file path="src/classes/worker.ts">
import { EventEmitter } from "node:events";
import { Job } from "./job";
import type { RedisClient, WorkerOptions } from "../interfaces";
import {
  createRedisClient,
  delay,
  getQueueKeys,
  type QueueKeys,
} from "../utils";
import { LuaScripts } from "../scripts/lua";
import { randomUUID } from "node:crypto";

export type Processor<
  TData = any,
  TResult = any,
  TName extends string = string
> = (job: Job<TData, TResult, TName>) => Promise<TResult>;

export class Worker<
  TData = any,
  TResult = any,
  TName extends string = string
> extends EventEmitter {
  readonly name: string;
  readonly client: RedisClient;
  readonly bClient: RedisClient; // Blocking client
  readonly opts: WorkerOptions;
  readonly keys: QueueKeys;
  private scripts: LuaScripts;
  private processor: Processor<TData, TResult, TName>;
  private prefix: string;

  private running = false;
  private closing: Promise<void> | null = null;
  private paused = false;
  private jobsInFlight = new Map<string, Job<TData, TResult, TName>>(); // Track active jobs
  private lockRenewTimers = new Map<string, NodeJS.Timeout>();
  private mainLoopPromise: Promise<void> | null = null;
  private workerId = randomUUID(); // Unique ID for this worker instance

  constructor(
    name: string,
    processor: Processor<TData, TResult, TName>,
    opts: WorkerOptions
  ) {
    super();
    this.name = name;
    this.opts = {
      concurrency: 1,
      lockDuration: 30000,
      lockRenewTime: 15000,
      ...opts,
    };
    this.prefix = opts.prefix ?? "lightq";
    this.keys = getQueueKeys(this.prefix, this.name);

    this.client = createRedisClient(opts.connection);
    this.bClient = this.client.duplicate();

    this.scripts = new LuaScripts(this.client); // Scripts use the normal client
    this.processor = processor;

    this.client.on("error", (err) => this.emit("error", err));
    this.bClient.on("error", (err) => this.emit("error", err)); // Listen to blocking client errors too

    this.run(); // Autorun by default (can be made optional)
  }

  private async run(): Promise<void> {
    if (this.running || this.closing) {
      return;
    }
    this.running = true;
    this.mainLoopPromise = this.mainLoop();
  }

  private async mainLoop(): Promise<void> {
    while (this.running && !this.closing && !this.paused) {
      if (this.jobsInFlight.size < (this.opts.concurrency ?? 1)) {
        try {
          const job = await this.acquireNextJob();
          if (job) {
            // Process concurrently but don't await here directly
            this.processJob(job).catch((err) => {
              console.error(`Unhandled error processing job ${job.id}:`, err);
              // Ensure cleanup even on unhandled promise rejection
              this.cleanupJob(job.id, err);
            });
          } else {
            // No job found, wait a bit before retrying to avoid busy-looping
            await delay(1000); // Configurable delay?
          }
        } catch (err) {
          if (
            !this.closing &&
            (err as Error).message !== "Connection is closed."
          ) {
            this.emit("error", err as Error);
            // Wait before retrying after an error
            await delay(5000);
          }
        }
      } else {
        // At concurrency limit, wait a bit
        await delay(200); // Small delay to yield
      }
    }
    this.running = false;
    // Ensure cleanup if loop exits unexpectedly
    await this.waitForJobsToComplete();
  }

  private async acquireNextJob(): Promise<Job<TData, TResult, TName> | null> {
    // TODO: Incorporate blocking B(L|Z)MOVE or BRPOPLPUSH equivalent via Lua
    // This simple version uses polling for delayed jobs and non-blocking moves.
    // A production version NEEDS blocking for efficiency.

    // 1. Check delayed jobs
    const now = Date.now();
    const movedFromDelayed = await this.scripts.moveDelayedToWait(
      this.keys,
      now
    );
    if (movedFromDelayed > 0) {
      this.emit("movedDelayed", movedFromDelayed); // Maybe a local event?
    }

    // 2. Attempt to move from wait to active (needs Lua script)
    const lockToken = randomUUID();
    const lockDuration = this.opts.lockDuration!;
    const result = await this.scripts.moveToActive(
      this.keys,
      lockToken,
      lockDuration
    );

    if (result) {
      const [jobId, rawJobData] = result;
      if (jobId && rawJobData) {
        // Got a job!
        // Construct Job from raw data (needs parsing logic in Job class)
        const job = Job.fromRedisHash<TData, TResult, TName>(
          // Need a Queue-like object or pass necessary info
          { client: this.client, keys: this.keys, name: this.name } as any,
          rawJobData
        );
        job.lockToken = lockToken;
        job.lockedUntil = Date.now() + lockDuration;

        this.jobsInFlight.set(job.id, job);
        this.startLockRenewal(job);
        this.emit("active", job);
        return job;
      }
    }

    // No job found in wait
    return null;
  }

  private async processJob(job: Job<TData, TResult, TName>): Promise<void> {
    try {
      const result = await this.processor(job);

      // *** FIX START: Check moveToCompleted result ***
      // Check if the job is still considered active by this worker before moving
      // This helps prevent attempting to complete a job whose lock was lost (and cleaned up)
      if (!this.jobsInFlight.has(job.id)) {
        console.warn(
          `Job ${job.id} processing finished, but it was no longer tracked as active (lock likely lost). Skipping move to completed.`
        );
        return; // Exit early, cleanup already happened or will happen
      }

      const removeOpt =
        job.opts.removeOnComplete ??
        this.opts.removeOnComplete ??
        (this as any)._queue?.opts?.defaultJobOptions?.removeOnComplete ??
        false;

      const moveResult = await this.scripts.moveToCompleted(
        this.keys,
        job,
        result,
        removeOpt ?? false
      );
      if (moveResult === 0) {
        // Successfully moved
        this.emit("completed", job, result);
        this.cleanupJob(job.id); // Cleanup only after successful move
      } else {
        // Failed to move (e.g., lock mismatch - code -1, or not in active list - code -2)
        console.error(
          `Failed to move job ${job.id} to completed state (Redis script code: ${moveResult}). Lock may have been lost or job state inconsistent.`
        );
        // Even if move fails, we need to clean up the job locally
        this.cleanupJob(job.id);
        // Optionally emit a different event like 'error' or 'stalled' here?
        this.emit(
          "error",
          new Error(
            `Failed to move job ${job.id} to completed (Script Code: ${moveResult})`
          ),
          job
        );
      }
    } catch (err) {
      // Check if job is still valid before handling error
      if (!this.jobsInFlight.has(job.id)) {
        console.warn(
          `Job ${job.id} processing failed, but it was no longer tracked as active (lock likely lost). Skipping move to failed/retry.`
        );
        return; // Exit early
      }
      this.handleJobError(job, err as Error); // handleJobError will call cleanupJob
    }
  }

  private async handleJobError(
    job: Job<TData, TResult, TName>,
    err: Error
  ): Promise<void> {
    job.attemptsMade += 1;
    const opts = job.opts;
    const maxAttempts = opts.attempts ?? 1;

    // Emit failed *before* attempting state change, so listeners know about the processor error

    this.emit("failed", job, err);

    let moveSuccessful = false; // Track if Redis move succeeds

    try {
      if (job.attemptsMade < maxAttempts) {
        // Calculate backoff
        let backoffDelay = 0;
        if (opts.backoff) {
          if (typeof opts.backoff === "number") {
            backoffDelay = opts.backoff;
          } else if (opts.backoff.type === "fixed") {
            backoffDelay = opts.backoff.delay;
          } else if (opts.backoff.type === "exponential") {
            backoffDelay = Math.round(
              opts.backoff.delay * Math.pow(2, job.attemptsMade - 1)
            );
          }
        }
        // Retry: Move to delayed
        const retryResult = await this.scripts.retryJob(
          this.keys,
          job,
          backoffDelay,
          err
        );
        if (retryResult === 0) {
          this.emit("retrying", job, err);
          moveSuccessful = true;
        } else {
          console.error(
            `Failed to move job ${job.id} for retry (Redis script code: ${retryResult}).`
          );
          // Don't emit retrying if move failed
        }
      } else {
        const removeOnFail =
          job.opts.removeOnFail ??
          this.opts.removeOnFail ??
          (this as any)._queue?.opts?.defaultJobOptions?.removeOnFail ??
          false;
        // Final failure: Move to failed
        const failedResult = await this.scripts.moveToFailed(
          this.keys,
          job,
          err,
          removeOnFail ?? false
        );
        if (failedResult === 0) {
          // Optionally emit a 'finallyFailed' event here if needed?
          moveSuccessful = true;
        } else {
          console.error(
            `Failed to move job ${job.id} to final failed state (Redis script code: ${failedResult}).`
          );
        }
      }
    } catch (redisError) {
      console.error(
        `Redis error during job failure handling for job ${job.id}:`,
        redisError
      );
      // Emit worker error?
      this.emit("error", redisError as Error, job);
    } finally {
      this.cleanupJob(job.id, err); // Pass error for context
    }
  }

  private startLockRenewal(job: Job<TData, TResult, TName>): void {
    const renewTime = this.opts.lockRenewTime ?? this.opts.lockDuration! / 2;
    const timer = setTimeout(async () => {
      if (this.jobsInFlight.has(job.id) && !this.closing && !this.paused) {
        try {
          const newLockedUntil = await this.scripts.extendLock(
            this.keys,
            job.id,
            job.lockToken!,
            this.opts.lockDuration!
          );
          if (newLockedUntil > 0) {
            job.lockedUntil = newLockedUntil;
            this.lockRenewTimers.delete(job.id); // Clear old timer before setting new one
            this.startLockRenewal(job); // Reschedule renewal
          } else {
            // Lock lost or job gone
            console.warn(
              `Failed to renew lock for job ${job.id}. It might have been stalled.`
            );
            this.cleanupJob(job.id); // Stop processing this job
          }
        } catch (err) {
          console.error(`Error renewing lock for job ${job.id}:`, err);
          // If renewal fails, we might lose the lock, stop processing
          this.cleanupJob(job.id);
        }
      }
    }, renewTime);
    this.lockRenewTimers.set(job.id, timer);
  }

  private cleanupJob(jobId: string, error?: Error): void {
    const timer = this.lockRenewTimers.get(jobId);
    if (timer) {
      clearTimeout(timer);
      this.lockRenewTimers.delete(jobId);
    }
    this.jobsInFlight.delete(jobId);
    // Potentially emit a specific 'jobCleaned' or 'jobFinishedProcessing' event
  }

  private async waitForJobsToComplete(): Promise<void> {
    const finishingPromises: Promise<any>[] = [];
    // NOTE: This is a simplified wait. In reality, you might need
    // a more robust mechanism, perhaps tracking promises directly.
    // This assumes that when `processJob` completes (or errors),
    // the job is eventually removed from jobsInFlight.
    while (this.jobsInFlight.size > 0) {
      console.log(`Waiting for ${this.jobsInFlight.size} jobs to complete...`);
      await delay(500); // Check periodically
    }
  }

  async close(force = false): Promise<void> {
    if (!this.closing) {
      this.closing = (async () => {
        this.emit("closing");
        this.running = false; // Signal loops to stop

        // Wait for the main loop to potentially finish its current iteration
        if (this.mainLoopPromise) {
          try {
            // Give the main loop a brief moment to exit if it's in a delay/wait
            await Promise.race([this.mainLoopPromise, delay(force ? 50 : 500)]);
          } catch (e) {
            /* Ignore errors during close */
            console.warn("Error ignored during main loop shutdown:", e);
          }
        }
        this.mainLoopPromise = null; // Ensure we don't await it again

        if (!force) {
          await this.waitForJobsToComplete();
        } else {
          console.log(
            `Force closing: Skipping wait for ${this.jobsInFlight.size} jobs.`
          );
          // Optionally try to release locks for jobsInFlight? Risky during force close.
        }

        // Clear all pending lock renewals
        this.lockRenewTimers.forEach((timer) => clearTimeout(timer));
        this.lockRenewTimers.clear();

        try {
          // *** FIX START: Use disconnect() on force close ***
          if (force) {
            console.log("Force closing Redis connections using disconnect().");
            // Disconnect immediately without waiting for server ACK
            this.client.disconnect();
            this.bClient.disconnect();
          } else {
            // Attempt graceful shutdown
            await Promise.all([
              this.client.quit(),
              this.bClient.quit(), // Close blocking client too
            ]);
          }
          // *** FIX END ***
        } catch (err) {
          // Ignore "Connection is closed" error which is expected if already closed/disconnected
          if (
            (err as Error).message &&
            !(err as Error).message.includes("Connection is closed")
          ) {
            console.error("Error during Redis connection close:", err);
            // Force disconnect if quit/disconnect failed unexpectedly
            try {
              this.client.disconnect();
            } catch {
              /* ignore */
            }
            try {
              this.bClient.disconnect();
            } catch {
              /* ignore */
            }
          }
        }
        this.jobsInFlight.clear(); // Ensure jobsInFlight is cleared on close
        this.emit("closed");
      })();
    }
    return this.closing;
  }
}
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    // Environment setup & latest features
    "lib": ["ESNext"],
    "target": "ESNext",
    "module": "ESNext",
    "moduleDetection": "force",
    "jsx": "react-jsx",
    "allowJs": true,

    // Bundler mode
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "emitDeclarationOnly": true,
    "declaration": true,
    "declarationDir": "./dist",

    // Best practices
    "strict": true,
    "skipLibCheck": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedIndexedAccess": true,

    // Some stricter flags (disabled by default)
    "noUnusedLocals": false,
    "noUnusedParameters": false,
    "noPropertyAccessFromIndexSignature": false,
    "stripInternal": true
  },
  "include": ["src/**/*"],
  "exclude": ["src/macros/loadLuaScript.ts"]
}
</file>

<file path="src/scripts/lua.ts">
import type { JobData, RedisClient } from "../interfaces";
import { Pipeline } from "ioredis";
import { Job } from "../classes/job";
import type { QueueKeys } from "../utils";
import { loadLuaScriptContent } from "../macros/loadLuaScript.ts" with { type: "macro" };

export class LuaScripts {
  private client: RedisClient;

  constructor(client: RedisClient) {
    this.client = client;
    this.loadScripts();
  }

  private loadScripts() {
    this.client.defineCommand("addJob", {
      numberOfKeys: 3,
      lua: loadLuaScriptContent("addJob"),
    });

    // moveToActive
    this.client.defineCommand("moveToActive", {
      numberOfKeys: 3,
      lua: loadLuaScriptContent("moveToActive"),
    });

    // moveToCompleted
    this.client.defineCommand("moveToCompleted", {
      numberOfKeys: 3,
      lua: loadLuaScriptContent("moveToCompleted"),
    });

    // moveToFailed
    this.client.defineCommand("moveToFailed", {
      numberOfKeys: 3,
      lua: loadLuaScriptContent("moveToFailed"),
    });

    // retryJob
    this.client.defineCommand("retryJob", {
      numberOfKeys: 4,
      lua: loadLuaScriptContent("retryJob"),
    });

    // moveDelayedToWait
    this.client.defineCommand("moveDelayedToWait", {
      numberOfKeys: 2,
      lua: loadLuaScriptContent("moveDelayedToWait"),
    });

    // extendLock
    this.client.defineCommand("extendLock", {
      numberOfKeys: 1,
      lua: loadLuaScriptContent("extendLock"),
    });
  }

  async addJob(
    keys: QueueKeys,
    jobData: JobData<any>,
    pipeline?: Pipeline,
  ): Promise<string> {
    const command = pipeline || this.client;
    const optsJson = JSON.stringify(jobData.opts);
    const dataJson = JSON.stringify(jobData.data);

    const args = [
      jobData.id,
      jobData.name,
      dataJson,
      optsJson,
      jobData.timestamp.toString(),
      jobData.delay.toString(),
      jobData.attemptsMade.toString(),
    ];
    // @ts-ignore
    return command.addJob(keys.jobs, keys.wait, keys.delayed, ...args);
  }

  async moveToActive(
    keys: QueueKeys,
    lockToken: string,
    lockDuration: number,
  ): Promise<[string, Record<string, string>] | null> {
    const now = Date.now();
    const args = [lockToken, lockDuration.toString(), now.toString()];
    // @ts-ignore
    const result = await this.client.moveToActive(
      keys.wait,
      keys.active,
      keys.jobs,
      ...args,
    );
    if (result) {
      const jobId = result[0];
      const jobDataArr = result[1];
      const jobDataMap: Record<string, string> = {};
      for (let i = 0; i < jobDataArr.length; i += 2) {
        jobDataMap[jobDataArr[i]] = jobDataArr[i + 1];
      }
      return [jobId, jobDataMap];
    }
    return null;
  }

  async moveToCompleted(
    keys: QueueKeys,
    job: Job<any, any, any>,
    returnValue: any,
    removeOnComplete: boolean | number,
  ): Promise<number> {
    const now = Date.now();
    const removeOption = typeof removeOnComplete === "number"
      ? removeOnComplete.toString()
      : String(removeOnComplete);
    const rvJson = JSON.stringify(returnValue ?? null);
    const args = [job.id, rvJson, removeOption, now.toString(), job.lockToken!];
    // @ts-ignore
    return this.client.moveToCompleted(
      keys.active,
      keys.completed,
      keys.jobs,
      ...args,
    );
  }

  async moveToFailed(
    keys: QueueKeys,
    job: Job<any, any, any>,
    error: Error,
    removeOnFail: boolean | number,
  ): Promise<number> {
    const now = Date.now();
    const removeOption = typeof removeOnFail === "number"
      ? removeOnFail.toString()
      : String(removeOnFail);
    const failedReason = error.message || "Unknown error";
    const stacktrace = JSON.stringify(
      error.stack?.split("\n").slice(0, 20) ?? [],
    );
    const finalAttemptsMade = job.attemptsMade;
    const args = [
      job.id,
      failedReason,
      stacktrace,
      removeOption,
      now.toString(),
      job.lockToken!,
      finalAttemptsMade.toString(),
    ];
    // @ts-ignore
    return this.client.moveToFailed(
      keys.active,
      keys.failed,
      keys.jobs,
      ...args,
    );
  }

  async retryJob(
    keys: QueueKeys,
    job: Job<any, any, any>,
    delay: number,
    error: Error,
  ): Promise<number> {
    const now = Date.now();
    const failedReason = error.message || "Retry Error";
    const stacktrace = JSON.stringify(
      error.stack?.split("\n").slice(0, 20) ?? [],
    );
    const args = [
      job.id,
      delay.toString(),
      now.toString(),
      failedReason,
      stacktrace,
    ];
    // @ts-ignore
    return this.client.retryJob(
      keys.active,
      keys.delayed,
      keys.wait,
      keys.jobs,
      ...args,
    );
  }

  async moveDelayedToWait(
    keys: QueueKeys,
    timestamp: number,
    limit: number = 50,
  ): Promise<number> {
    const args = [timestamp.toString(), limit.toString()];
    // @ts-ignore
    return this.client.moveDelayedToWait(keys.delayed, keys.wait, ...args);
  }

  async extendLock(
    keys: QueueKeys,
    jobId: string,
    token: string,
    duration: number,
  ): Promise<number> {
    // jobsPrefix is not needed directly, jobKey is constructed in Lua
    // const jobKey = `${keys.jobs}:${jobId}`; // Constructing jobKey here is not needed for the call
    const now = Date.now();
    const args = [jobId, token, duration.toString(), now.toString()];
    // @ts-ignore
    return this.client.extendLock(keys.jobs, ...args);
  }
}
</file>

<file path="test/smq.test.ts">
import {
  afterAll,
  afterEach,
  beforeAll,
  beforeEach,
  describe,
  expect,
  it,
  mock,
  spyOn,
} from "bun:test";
import IORedis, { Pipeline, type Redis } from "ioredis";
import {
  Job,
  type JobData,
  JobScheduler,
  JobTemplate,
  type Processor,
  Queue,
  RedisClient,
  SchedulerRepeatOptions,
  Worker,
} from "../src";
import process from "node:process";
import * as queueUtils from "../src/utils";
import { Cron } from "croner";

const REDIS_HOST = process.env.REDIS_HOST || "localhost";
const REDIS_PORT = process.env.REDIS_PORT
  ? parseInt(process.env.REDIS_PORT, 10)
  : 6379;
const redisConnectionOpts = {
  host: REDIS_HOST,
  port: REDIS_PORT,
  maxRetriesPerRequest: null,
};

const delay = (ms: number): Promise<void> =>
  new Promise((resolve) => setTimeout(resolve, ms));

async function waitFor(
  conditionFn: () => Promise<boolean> | boolean,
  timeout = 5000,
  interval = 100
): Promise<void> {
  const start = Date.now();
  while (Date.now() - start < timeout) {
    if (await conditionFn()) {
      return;
    }
    await delay(interval);
  }
  throw new Error(`Condition not met within ${timeout}ms`);
}

describe("LightQ (lightq)", () => {
  let testQueueName: string;
  let redisClient: Redis;
  let queuesToClose: Queue<any, any, any>[] = [];
  let workersToClose: Worker<any, any, any>[] = [];
  let schedulersToClose: JobScheduler[] = [];

  // Helper to create unique queue names for each test
  const generateQueueName = (base = "test-queue") =>
    `${base}:${Date.now()}:${Math.random().toString(36).substring(7)}`;

  // Helper to create and track queues
  const createQueue = <
    TData = any,
    TResult = any,
    TName extends string = string
  >(
    name: string,
    opts: Partial<Queue<TData, TResult, TName>["opts"]> = {}
  ): Queue<TData, TResult, TName> => {
    const queue = new Queue<TData, TResult, TName>(name, {
      connection: { ...redisConnectionOpts }, // Use a fresh connection object potentially
      ...opts,
    });
    queuesToClose.push(queue);
    return queue;
  };

  const createScheduler = (
    queue: Queue<any, any, any>,
    opts: Partial<JobScheduler["opts"]> = {}
  ): JobScheduler => {
    // Use the internal getScheduler method for consistency if possible,
    // otherwise instantiate directly for isolated tests.
    // Direct instantiation:
    const scheduler = new JobScheduler(queue, {
      connection: queue.client, // Share client
      prefix: queue.opts.prefix,
      checkInterval: 100, // Faster checks for tests
      ...opts,
    });
    schedulersToClose.push(scheduler);
    return scheduler;
  };

  // Helper to create and track workers
  const createWorker = <
    TData = any,
    TResult = any,
    TName extends string = string
  >(
    name: string,
    processor: Processor<TData, TResult, TName>,
    opts: Partial<Worker<TData, TResult, TName>["opts"]> = {}
  ): Worker<TData, TResult, TName> => {
    const worker = new Worker<TData, TResult, TName>(name, processor, {
      connection: { ...redisConnectionOpts }, // Use a fresh connection object potentially
      lockDuration: 5000, // Shorter duration for tests
      lockRenewTime: 2500,
      ...opts,
    });
    workersToClose.push(worker);
    return worker;
  };

  beforeAll(async () => {
    redisClient = new IORedis(redisConnectionOpts);
    // Ensure connection is ready before tests start
    await redisClient.ping();
  });

  afterAll(async () => {
    await redisClient.quit();
  });

  beforeEach(async () => {
    testQueueName = generateQueueName();
    queuesToClose = [];
    workersToClose = [];
    schedulersToClose = [];
    // Clear Redis before each test
    await redisClient.flushdb();
    mock.restore();
    spyOn(Date, "now");
  });

  afterEach(async () => {
    mock.restore();
    await Promise.all(
      schedulersToClose.map((s) =>
        (s as any) // Access private closing state if needed for robustness check
          .close()
          .catch((e) =>
            console.error(
              `Error closing scheduler for queue ${
                (s as any).queue.name // Access private queue ref
              }: ${e.message}`
            )
          )
      )
    );
    // Close all created queues and workers
    await Promise.all([
      ...workersToClose.map((w) =>
        w
          .close()
          .catch((e) =>
            console.error(`Error closing worker ${w.name}: ${e.message}`)
          )
      ), // Force close workers quickly
      ...queuesToClose.map((q) =>
        q
          .close()
          .catch((e) =>
            console.error(`Error closing queue ${q.name}: ${e.message}`)
          )
      ),
    ]);
    workersToClose = [];
    queuesToClose = [];
    schedulersToClose = [];
    mock.restore();
  });

  describe("Job Class", () => {
    it("should create Job instance from data", () => {
      const queue = createQueue(testQueueName);
      const jobData: JobData = {
        id: "job-1",
        name: "test-job",
        data: { x: 1 },
        opts: { attempts: 1, delay: 0 },
        timestamp: Date.now(),
        delay: 0,
        attemptsMade: 0,
      };
      const job = Job.fromData(queue, jobData);
      expect(job).toBeInstanceOf(Job);
      expect(job.id).toBe("job-1");
      expect(job.data).toEqual({ x: 1 });
    });

    it("should convert Job instance back to data", () => {
      const queue = createQueue(testQueueName);
      const jobDataInput: JobData = {
        id: "job-2",
        name: "test-job-2",
        data: { y: "abc" },
        opts: { attempts: 3, delay: 100 },
        timestamp: Date.now(),
        delay: 100,
        attemptsMade: 1,
        processedOn: Date.now() - 1000,
        finishedOn: Date.now(),
        returnValue: { result: "ok" },
        failedReason: undefined,
        stacktrace: [],
        lockedUntil: undefined,
        lockToken: undefined,
      };
      const job = Job.fromData(queue, jobDataInput);
      const jobDataOutput = job.toData();

      // Compare relevant fields (ignore potential minor timestamp differences if applicable)
      expect(jobDataOutput.id).toBe(jobDataInput.id);
      expect(jobDataOutput.name).toBe(jobDataInput.name);
      expect(jobDataOutput.data).toEqual(jobDataInput.data);
      expect(jobDataOutput.opts).toEqual(jobDataInput.opts);
      expect(jobDataOutput.timestamp).toBe(jobDataInput.timestamp);
      expect(jobDataOutput.delay).toBe(jobDataInput.delay);
      expect(jobDataOutput.attemptsMade).toBe(jobDataInput.attemptsMade);
      expect(jobDataOutput.processedOn).toBe(jobDataInput.processedOn!);
      expect(jobDataOutput.finishedOn).toBe(jobDataInput.finishedOn!);
      expect(jobDataOutput.returnValue).toEqual(jobDataInput.returnValue);
    });

    it("should parse job from Redis hash correctly", async () => {
      const queue = createQueue(testQueueName);
      const jobId = "job-from-redis";
      const timestamp = Date.now();
      const hashData = {
        id: jobId,
        name: "redis-job",
        data: JSON.stringify({ value: 123 }),
        opts: JSON.stringify({ attempts: 2, removeOnFail: true }),
        timestamp: timestamp.toString(),
        delay: "500",
        attemptsMade: "1",
        processedOn: (timestamp + 1000).toString(),
        finishedOn: (timestamp + 2000).toString(),
        returnValue: JSON.stringify({ success: true }),
        failedReason: "Something failed",
        stacktrace: JSON.stringify(["line 1", "line 2"]),
        lockedUntil: (timestamp + 5000).toString(),
        lockToken: "redis-lock-token",
      };
      await redisClient.hset(`${queue.keys.jobs}:${jobId}`, hashData);

      // Use queue.getJob which uses fromRedisHash internally
      const job = await queue.getJob(jobId);

      expect(job).toBeInstanceOf(Job);
      expect(job!.id).toBe(jobId);
      expect(job!.name).toBe("redis-job");
      expect(job!.data).toEqual({ value: 123 });
      expect(job!.opts).toEqual({ attempts: 2, removeOnFail: true });
      expect(job!.timestamp).toBe(timestamp);
      expect(job!.delay).toBe(500);
      expect(job!.attemptsMade).toBe(1);
      expect(job!.processedOn).toBe(timestamp + 1000);
      expect(job!.finishedOn).toBe(timestamp + 2000);
      expect(job!.returnValue).toEqual({ success: true });
      expect(job!.failedReason).toBe("Something failed");
      expect(job!.stacktrace).toEqual(["line 1", "line 2"]);
      expect(job!.lockedUntil).toBe(timestamp + 5000);
      expect(job!.lockToken).toBe("redis-lock-token");
    });

    it("should handle missing optional fields when parsing from Redis hash", async () => {
      const queue = createQueue(testQueueName);
      const jobId = "job-missing-fields";
      const timestamp = Date.now();
      const hashData = {
        // Only required fields + some others
        id: jobId,
        name: "minimal-job",
        data: JSON.stringify({}),
        opts: JSON.stringify({}),
        timestamp: timestamp.toString(),
        delay: "0",
        attemptsMade: "0",
      };
      await redisClient.hset(`${queue.keys.jobs}:${jobId}`, hashData);

      const job = await queue.getJob(jobId);

      expect(job).toBeInstanceOf(Job);
      expect(job!.id).toBe(jobId);
      expect(job!.processedOn).toBeUndefined();
      expect(job!.finishedOn).toBeUndefined();
      expect(job!.returnValue).toBeUndefined();
      expect(job!.failedReason).toBeUndefined();
      expect(job!.stacktrace).toEqual([]); // Default value
      expect(job!.lockedUntil).toBeUndefined();
      expect(job!.lockToken).toBeUndefined();
    });

    it("should warn when updateProgress is called (as it's not implemented)", async () => {
      const queue = createQueue(testQueueName);
      const job = await queue.add("progress-job", { d: 1 });
      const consoleWarnSpy = spyOn(console, "warn"); // Spy on console.warn

      await job.updateProgress(50);

      expect(consoleWarnSpy).toHaveBeenCalledTimes(1);
      expect(consoleWarnSpy).toHaveBeenCalledWith(
        "updateProgress not fully implemented in this simple version"
      );

      // Optional: Check if HSET was attempted (it shouldn't ideally if not impl)
      // This requires more involved mocking or checking Redis state changes
      // For now, just checking the warning is sufficient given the implementation note.
    });
  });

  describe("Queue Class", () => {
    it("should create a queue instance", () => {
      const queue = createQueue(testQueueName);
      expect(queue).toBeInstanceOf(Queue);
      expect(queue.name).toBe(testQueueName);
    });

    it("should add a job to the wait list", async () => {
      const queue = createQueue<{ msg: string }>(testQueueName);
      const jobData = { msg: "hello" };
      const jobName = "test-job";

      const job = await queue.add(jobName, jobData);

      expect(job).toBeInstanceOf(Job);
      expect(job.id).toBeString();
      expect(job.name).toBe(jobName);
      expect(job.data).toEqual(jobData);
      expect(job.opts.attempts).toBe(1); // Default

      const counts = await queue.getJobCounts();
      expect(counts.wait).toBe(1);
      expect(counts.active).toBe(0);
      expect(counts.completed).toBe(0);
      expect(counts.failed).toBe(0);
      expect(counts.delayed).toBe(0);

      const retrievedJob = await queue.getJob(job.id);
      expect(retrievedJob).not.toBeNull();
      expect(retrievedJob!.id).toBe(job.id);
      expect(retrievedJob!.data).toEqual(jobData);
    });

    it("should add a delayed job to the delayed list", async () => {
      const queue = createQueue<{ msg: string }>(testQueueName);
      const jobData = { msg: "later" };
      const jobName = "delayed-job";
      const delayMs = 500;

      const job = await queue.add(jobName, jobData, { delay: delayMs });

      let counts = await queue.getJobCounts();
      expect(counts.delayed).toBe(1);
      expect(counts.wait).toBe(0);

      // Wait for the job to become active (needs worker or manual script call simulation)
      // We'll test the move script directly for simplicity here
      const keys = queue.keys;
      await delay(delayMs + 100); // Wait past the delay
      // @ts-ignore - Accessing private scripts for testing moveDelayedToWait
      const movedCount = await queue.scripts.moveDelayedToWait(
        keys,
        Date.now(),
        10
      );

      expect(movedCount).toBe(1);

      counts = await queue.getJobCounts();
      expect(counts.delayed).toBe(0);
      expect(counts.wait).toBe(1);

      const retrievedJob = await queue.getJob(job.id);
      expect(retrievedJob).not.toBeNull();
    });

    it("should add jobs in bulk", async () => {
      const queue = createQueue<{ index: number }>(testQueueName);
      const jobsToAdd = [
        { name: "bulk-job", data: { index: 1 } },
        { name: "bulk-job", data: { index: 2 }, opts: { delay: 100 } },
        { name: "bulk-job", data: { index: 3 } },
      ];

      const addedJobs = await queue.addBulk(jobsToAdd);

      expect(addedJobs).toHaveLength(3);
      expect(addedJobs[0]).toBeInstanceOf(Job);
      expect(addedJobs[1]?.opts.delay).toBe(100);

      const counts = await queue.getJobCounts();
      expect(counts.wait).toBe(2);
      expect(counts.delayed).toBe(1);

      // Verify jobs exist
      const job1 = await queue.getJob(addedJobs[0]!.id);
      const job2 = await queue.getJob(addedJobs[1]!.id);
      expect(job1).not.toBeNull();
      expect(job2).not.toBeNull();
      expect(job1?.data.index).toBe(1);
      expect(job2?.data.index).toBe(2);
    });

    it("should apply default job options", async () => {
      const queue = createQueue<{ msg: string }>(testQueueName, {
        defaultJobOptions: { attempts: 5, removeOnComplete: true },
      });
      const job = await queue.add("default-opts-job", { msg: "test" });

      expect(job.opts.attempts).toBe(5);
      expect(job.opts.removeOnComplete).toBe(true);

      const retrievedJob = await queue.getJob(job.id);
      expect(retrievedJob!.opts.attempts).toBe(5);
      expect(retrievedJob!.opts.removeOnComplete).toBe(true);
    });

    it("should override default job options", async () => {
      const queue = createQueue<{ msg: string }>(testQueueName, {
        defaultJobOptions: { attempts: 5, removeOnComplete: true },
      });
      const job = await queue.add(
        "override-opts-job",
        { msg: "test" },
        {
          attempts: 2,
          removeOnComplete: false,
        }
      );

      expect(job.opts.attempts).toBe(2);
      expect(job.opts.removeOnComplete).toBe(false);

      const retrievedJob = await queue.getJob(job.id);
      expect(retrievedJob!.opts.attempts).toBe(2);
      expect(retrievedJob!.opts.removeOnComplete).toBe(false);
    });

    it("should retrieve job counts accurately", async () => {
      const queue = createQueue(testQueueName);
      await queue.add("job1", { d: 1 });
      await queue.add("job2", { d: 2 }, { delay: 5000 }); // delayed
      // Manually move one to active (simulate worker start) - Requires internal script access
      // For a pure queue test, we rely on adding to specific lists initially.
      // We will test counts more thoroughly with the worker.

      const counts = await queue.getJobCounts();
      expect(counts.wait).toBe(1);
      expect(counts.delayed).toBe(1);
      expect(counts.active).toBe(0);
      expect(counts.completed).toBe(0);
      expect(counts.failed).toBe(0);
    });

    it("should close the queue connection", async () => {
      const queue = createQueue(testQueueName);
      await queue.add("job-before-close", { d: 1 });
      await queue.close();
      // Try adding after close - should throw
      await expect(queue.add("job-after-close", { d: 2 })).rejects.toThrow(
        "Queue is closing"
      );
      // Check connection status (ioredis specific)
      // expect(queue.client.status).toBe("end");
      // Remove from list so afterEach doesn't try to close again
      queuesToClose = queuesToClose.filter((q) => q !== queue);
    });

    it("should not add a job if job ID already exists", async () => {
      const queue = createQueue(testQueueName);
      const jobId = "duplicate-job-id";
      const job1 = await queue.add("job-type-1", { val: 1 }, { jobId });
      const initialCounts = await queue.getJobCounts();
      expect(initialCounts.wait).toBe(1);

      // Try adding again with the same jobId
      // NOTE: The current implementation of queue.add doesn't check the script's
      // return value (0 for duplicate). It returns a Job object regardless.
      // The *effect* however, is that the job data isn't overwritten and
      // the job isn't added to the wait/delayed list again by the Lua script.
      const job2 = await queue.add("job-type-2", { val: 2 }, { jobId });

      expect(job2.id).toBe(jobId); // It creates a Job object representation

      // Verify counts haven't changed
      const finalCounts = await queue.getJobCounts();
      expect(finalCounts.wait).toBe(1); // Should still be 1
      expect(finalCounts.delayed).toBe(0);

      // Verify the data in Redis hasn't changed to the second job's data
      const retrievedJob = await queue.getJob(jobId);
      expect(retrievedJob).not.toBeNull();
      expect(retrievedJob!.name).toBe("job-type-1"); // Should be the first job's name
      expect(retrievedJob!.data).toEqual({ val: 1 }); // Should be the first job's data
    });

    it("should close the queue connection", async () => {
      const queue = createQueue(testQueueName);
      // Use a separate client to check connection status if possible,
      // or rely on the error throwing behavior.
      const connectionStatusClient = new IORedis({ ...redisConnectionOpts });
      queuesToClose.push({
        client: connectionStatusClient,
        close: async () => {
          connectionStatusClient.disconnect();
        },
      } as any); // Add to cleanup

      const sharedQueue = new Queue(testQueueName, {
        connection: connectionStatusClient,
      });
      queuesToClose.push(sharedQueue);

      await sharedQueue.add("job-before-close", { d: 1 });
      let statusBefore = connectionStatusClient.status;
      expect(statusBefore).toBeOneOf(["connecting", "connect", "ready"]);

      await sharedQueue.close();

      // Wait a moment for status to potentially update
      await delay(50);
      let statusAfter = connectionStatusClient.status;
      // ioredis might go to 'end' or just stay 'ready' if other connections exist
      // console.log("Status after close:", statusAfter);

      // Try adding after close - should throw
      await expect(
        sharedQueue.add("job-after-close", { d: 2 })
      ).rejects.toThrow("Queue is closing");

      // Remove from list so afterEach doesn't try to close again
      // (Handled by pushing onto queuesToClose and the afterEach loop)
      queuesToClose = queuesToClose.filter((q) => q !== sharedQueue);
      queuesToClose = queuesToClose.filter(
        (q) => q.client !== connectionStatusClient
      ); // Remove the status checker too
      await connectionStatusClient.quit().catch(() => {}); // Ensure checker client is closed
    });

    it("should execute a raw script command via executeScript", async () => {
      const queue = createQueue(testQueueName);
      // Example: Use EVAL to run a simple command
      const script = "return redis.call('SET', KEYS[1], ARGV[1])";
      const key = `${queue.keys.base}:exec-test`;
      const value = "hello world";
      // Define the command temporarily for testing (or use EVAL directly if simpler)
      queue.client.defineCommand("testExecScript", {
        // <-- Command 1
        numberOfKeys: 1,
        lua: script,
      });

      // Test without pipeline
      // @ts-ignore - Accessing potentially private/internal method for testing
      let result = await queue.executeScript("testExecScript", [key], [value]);
      expect(result).toBe("OK");
      expect(await redisClient.get(key)).toBe(value);

      // Test with pipeline
      const key2 = `${queue.keys.base}:exec-test-pipe`;
      const value2 = "pipeline test";

      // *** FIX START: Define command BEFORE creating the pipeline ***
      queue.client.defineCommand("testExecScriptPipe", {
        // <-- Command 2
        numberOfKeys: 1,
        lua: script,
      });
      const pipeline = queue.client.pipeline(); // Create pipeline AFTER definition
      // *** FIX END ***

      // @ts-ignore
      queue.executeScript("testExecScriptPipe", [key2], [value2], pipeline); // Queue the command onto the pipeline
      const execResult = await pipeline.exec(); // Execute the pipeline

      // --- Assertions ---
      expect(execResult).toHaveLength(1); // <-- Should pass now
      expect(execResult).toBeArray();
      expect(execResult![0]).toBeArray();
      // The result structure from pipeline.exec is [[error, resultFromCommand1], [error, resultFromCommand2], ...]
      expect(execResult![0]![0]).toBeNull(); // No error for the command
      expect(execResult![0]![1]).toBe("OK"); // Result of the script command in pipeline
      expect(await redisClient.get(key2)).toBe(value2);

      // Cleanup test keys
      await redisClient.del(key, key2);
    });

    describe("Scheduler Integration", () => {
      it("should initialize and start the scheduler on first access via getScheduler", () => {
        const queue = createQueue(testQueueName);
        // @ts-ignore - Access private method
        expect(queue.scheduler).toBeNull();

        // @ts-ignore - Access private method
        const schedulerInstance = queue.getScheduler();

        expect(schedulerInstance).toBeInstanceOf(JobScheduler);
        // @ts-ignore - Access private property
        expect(queue.scheduler).toBe(schedulerInstance);
        // @ts-ignore - Access private property
        expect(schedulerInstance.running).toBe(true); // Should auto-start

        // @ts-ignore - Access private method
        const schedulerInstance2 = queue.getScheduler();
        expect(schedulerInstance2).toBe(schedulerInstance); // Should return same instance

        // Add scheduler to cleanup automatically via queue close
        // No need to add to schedulersToClose manually here
      });

      it("should upsert a job scheduler via the queue method", async () => {
        const queue = createQueue(testQueueName);
        const schedulerId = "q-upsert-test";
        const repeat: SchedulerRepeatOptions = { every: 5000 };
        const template: JobTemplate = {
          name: "scheduled-task",
          data: { x: 1 },
        };

        // Spy on the actual JobScheduler method
        const schedulerUpsertSpy = spyOn(
          JobScheduler.prototype,
          "upsertJobScheduler"
        ).mockResolvedValue(undefined); // Mock the underlying implementation

        await queue.upsertJobScheduler(schedulerId, repeat, template);

        // @ts-ignore - Ensure scheduler was initialized
        expect(queue.scheduler).toBeInstanceOf(JobScheduler);
        expect(schedulerUpsertSpy).toHaveBeenCalledTimes(1);
        expect(schedulerUpsertSpy).toHaveBeenCalledWith(
          schedulerId,
          repeat,
          template
        );

        schedulerUpsertSpy.mockRestore();
      });

      it("should remove a job scheduler via the queue method (when scheduler exists)", async () => {
        const queue = createQueue(testQueueName);
        const schedulerId = "q-remove-test";

        // Ensure scheduler is initialized first
        // @ts-ignore - Access private method
        const schedulerInstance = queue.getScheduler();
        const schedulerRemoveSpy = spyOn(
          schedulerInstance,
          "removeJobScheduler"
        ).mockResolvedValue(true);

        const result = await queue.removeJobScheduler(schedulerId);

        expect(result).toBe(true);
        expect(schedulerRemoveSpy).toHaveBeenCalledTimes(1);
        expect(schedulerRemoveSpy).toHaveBeenCalledWith(schedulerId);

        schedulerRemoveSpy.mockRestore();
      });

      it("should handle removing a job scheduler via the queue method (when scheduler not initialized)", async () => {
        const queue = createQueue(testQueueName);
        const schedulerId = "q-remove-test-no-init";
        const consoleWarnSpy = spyOn(console, "warn");

        // @ts-ignore - Verify scheduler is not initialized
        expect(queue.scheduler).toBeNull();

        const result = await queue.removeJobScheduler(schedulerId);

        expect(result).toBe(false);
        expect(consoleWarnSpy).toHaveBeenCalledWith(
          expect.stringContaining(
            "Attempted to remove scheduler before scheduler process was started"
          )
        );
        // @ts-ignore - Verify scheduler is still not initialized
        expect(queue.scheduler).toBeNull();

        consoleWarnSpy.mockRestore();
      });

      it("should forward scheduler 'error' event", async () => {
        const queue = createQueue(testQueueName);
        const schedulerError = new Error("Scheduler Test Error");
        let caughtError: Error | null = null;

        queue.on("scheduler_error", (err) => {
          caughtError = err;
        });

        // @ts-ignore - Initialize scheduler
        const scheduler = queue.getScheduler();
        // Manually emit error on the scheduler instance
        scheduler.emit("error", schedulerError);

        await delay(10); // Allow event propagation

        // @ts-ignore - Check if caughtError is the same as schedulerError
        expect(caughtError).toBe(schedulerError);
      });

      it("should forward scheduler 'job_added' event", async () => {
        const queue = createQueue(testQueueName);
        const schedulerId = "emitter-test";
        const fakeJob = { id: "fake-job-1", name: "fake" };
        let caughtSchedulerId: string | null = null;
        let caughtJob: any = null;

        queue.on("scheduler_job_added", (id, job) => {
          caughtSchedulerId = id;
          caughtJob = job;
        });

        // @ts-ignore - Initialize scheduler
        const scheduler = queue.getScheduler();
        scheduler.emit("job_added", schedulerId, fakeJob);

        await delay(10); // Allow event propagation

        // @ts-ignore - Check if caughtSchedulerId and caughtJob match the emitted values
        expect(caughtSchedulerId).toBe(schedulerId);
        expect(caughtJob).toBe(fakeJob);
      });

      it("should prevent adding jobs with reserved scheduler ID prefix", async () => {
        const queue = createQueue(testQueueName);
        const badJobId = "scheduler:my-id";

        await expect(
          queue.add("some-job", { d: 1 }, { jobId: badJobId })
        ).rejects.toThrow(
          `Cannot manually add job with reserved scheduler ID: ${badJobId}`
        );

        // Test bulk add as well
        await expect(
          queue.addBulk([
            { name: "other-job", data: { d: 2 }, opts: { jobId: badJobId } },
          ])
        ).resolves.toEqual([]); // Should skip the job and return empty array

        const counts = await queue.getJobCounts();
        expect(counts.wait).toBe(0);
      });
    });
  });

  describe("JobScheduler Class", () => {
    let queue: Queue;
    let scheduler: JobScheduler;
    let mockClient: RedisClient; // Use a dedicated mock client for fine control

    // Mock Redis methods used by scheduler
    let hsetSpy: any;
    let zaddSpy: any;
    let delSpy: any;
    let zremSpy: any;
    let zrangebyscoreSpy: any;
    let hgetallSpy: any;
    let multiSpy: any;
    let execSpy: any;

    beforeEach(() => {
      // Create a real queue instance
      queue = createQueue(testQueueName + "-sched");

      // Create a scheduler instance directly for isolated tests
      scheduler = new JobScheduler(queue, {
        connection: queue.client, // Use the queue's client
        checkInterval: 50, // Faster checks
        schedulerPrefix: "testprefix", // Custom prefix
      });
      schedulersToClose.push(scheduler); // Ensure cleanup

      // --- Mock Redis Client Methods on the *specific client instance* used by the scheduler ---
      mockClient = scheduler["client"]; // Access the internal client
      hsetSpy = spyOn(mockClient, "hset").mockResolvedValue(1); // Simulate success
      zaddSpy = spyOn(mockClient, "zadd").mockResolvedValue("1");
      delSpy = spyOn(mockClient, "del").mockResolvedValue(1);
      zremSpy = spyOn(mockClient, "zrem").mockResolvedValue(1);
      zrangebyscoreSpy = spyOn(mockClient, "zrangebyscore").mockResolvedValue(
        []
      ); // Default: no jobs due
      hgetallSpy = spyOn(mockClient, "hgetall").mockResolvedValue({}); // Default: not found
      // Mock multi/exec chain
      execSpy = mock().mockResolvedValue([[null, 1]]); // Mock successful exec
      multiSpy = spyOn(mockClient, "multi").mockImplementation(() => {
        const pipeline = new Pipeline(mockClient); // Create a real pipeline structure
        pipeline.hset = mock().mockReturnThis(); // Mock pipeline methods to be chainable
        pipeline.zadd = mock().mockReturnThis();
        pipeline.del = mock().mockReturnThis();
        pipeline.zrem = mock().mockReturnThis();
        pipeline.exec = execSpy; // Attach the mocked exec
        return pipeline as any; // Return the mocked pipeline
      });

      // Prevent scheduler from auto-starting timer for most tests
      // @ts-ignore Access private property
      scheduler.running = false;
      if (scheduler["checkTimer"]) {
        clearTimeout(scheduler["checkTimer"]);
        scheduler["checkTimer"] = null;
      }
    });

    it("should initialize with default and overridden options", () => {
      expect(scheduler).toBeInstanceOf(JobScheduler);
      // @ts-ignore
      expect(scheduler.opts.prefix).toBe("lightq"); // Inherited from queue options
      // @ts-ignore
      expect(scheduler.opts.schedulerPrefix).toBe("testprefix");
      // @ts-ignore
      expect(scheduler.opts.checkInterval).toBe(50);
      // @ts-ignore
      expect(scheduler.keys.base).toBe(`testprefix:${queue.name}:schedulers`);
    });

    describe("upsertJobScheduler", () => {
      it("should add a new cron job scheduler", async () => {
        const schedulerId = "cron-job-1";
        const repeat: SchedulerRepeatOptions = {
          pattern: "0 * * * *",
          tz: "UTC",
        };
        const template: JobTemplate = { name: "scheduledCron", data: { a: 1 } };
        const now = Date.now();
        const expectedNextRun = new Cron(repeat.pattern!, {
          timezone: repeat.tz,
        })
          .nextRun(new Date(now))!
          .getTime();

        (Date.now as any).mockReturnValue(now); // Fix time

        await scheduler.upsertJobScheduler(schedulerId, repeat, template);

        const expectedKey = `testprefix:${queue.name}:schedulers:${schedulerId}`;
        const expectedData = {
          id: schedulerId,
          type: "cron",
          value: repeat.pattern,
          tz: repeat.tz,
          nextRun: expectedNextRun.toString(),
          name: template.name,
          data: JSON.stringify(template.data ?? {}),
          opts: JSON.stringify(template.opts ?? {}),
        };

        expect(multiSpy).toHaveBeenCalledTimes(1);
        // Retrieve the pipeline instance from the multi spy call
        const pipelineInstance = multiSpy.mock.results[0].value;
        expect(pipelineInstance.hset).toHaveBeenCalledWith(
          expectedKey,
          expectedData
        );
        expect(pipelineInstance.zadd).toHaveBeenCalledWith(
          scheduler["keys"].index, // Access private key
          expectedNextRun,
          schedulerId
        );
        expect(execSpy).toHaveBeenCalledTimes(1);
      });

      it("should add a new 'every' job scheduler", async () => {
        const schedulerId = "every-job-1";
        const repeat: SchedulerRepeatOptions = { every: 10000 }; // 10 seconds
        const template: JobTemplate = {
          name: "scheduledEvery",
          data: { b: 2 },
        };
        const now = Date.now();
        const expectedNextRun = now + repeat.every!;

        (Date.now as any).mockReturnValue(now);

        await scheduler.upsertJobScheduler(schedulerId, repeat, template);

        const expectedKey = `testprefix:${queue.name}:schedulers:${schedulerId}`;
        const expectedData = {
          id: schedulerId,
          type: "every",
          value: repeat.every.toString(), // Stored as string
          tz: undefined, // Ensure tz is not set for 'every'
          nextRun: expectedNextRun.toString(),
          name: template.name,
          data: JSON.stringify(template.data ?? {}),
          opts: JSON.stringify(template.opts ?? {}),
        };

        expect(multiSpy).toHaveBeenCalledTimes(1);
        const pipelineInstance = multiSpy.mock.results[0].value;
        // Use expect.objectContaining because the actual call might have more keys initially
        expect(pipelineInstance.hset).toHaveBeenCalledWith(
          expectedKey,
          expect.objectContaining(expectedData)
        );
        expect(pipelineInstance.zadd).toHaveBeenCalledWith(
          scheduler["keys"].index,
          expectedNextRun,
          schedulerId
        );
        expect(execSpy).toHaveBeenCalledTimes(1);
      });

      it("should update an existing scheduler", async () => {
        const schedulerId = "update-job-1";
        const initialRepeat: SchedulerRepeatOptions = { every: 10000 };
        const initialTemplate: JobTemplate = { name: "initial" };
        const updatedRepeat: SchedulerRepeatOptions = {
          pattern: "*/5 * * * *",
        }; // Every 5 mins
        const updatedTemplate: JobTemplate = {
          name: "updated",
          data: { updated: true },
        };
        const now = Date.now();
        const expectedNextRun = new Cron(updatedRepeat.pattern!)
          .nextRun(new Date(now))!
          .getTime();

        (Date.now as any).mockReturnValue(now);

        // First upsert
        await scheduler.upsertJobScheduler(
          schedulerId,
          initialRepeat,
          initialTemplate
        );
        multiSpy.mockClear(); // Clear mocks for the second call
        execSpy.mockClear();

        // Second (updating) upsert
        await scheduler.upsertJobScheduler(
          schedulerId,
          updatedRepeat,
          updatedTemplate
        );

        const expectedKey = `testprefix:${queue.name}:schedulers:${schedulerId}`;
        const expectedData = {
          id: schedulerId,
          type: "cron",
          value: updatedRepeat.pattern,
          tz: undefined,
          nextRun: expectedNextRun.toString(),
          name: updatedTemplate.name,
          data: JSON.stringify(updatedTemplate.data ?? {}),
          opts: JSON.stringify(updatedTemplate.opts ?? {}),
        };

        expect(multiSpy).toHaveBeenCalledTimes(1);
        const pipelineInstance = multiSpy.mock.results[0].value;
        expect(pipelineInstance.hset).toHaveBeenCalledWith(
          expectedKey,
          expect.objectContaining(expectedData)
        );
        expect(pipelineInstance.zadd).toHaveBeenCalledWith(
          scheduler["keys"].index,
          expectedNextRun,
          schedulerId
        );
        expect(execSpy).toHaveBeenCalledTimes(1);
      });

      it("should throw error for invalid repeat options", async () => {
        const schedulerId = "invalid-repeat";
        await expect(
          // @ts-ignore - Testing invalid input
          scheduler.upsertJobScheduler(schedulerId, {}, { name: "test" })
        ).rejects.toThrow("Invalid repeat options");
        await expect(
          scheduler.upsertJobScheduler(
            schedulerId,
            // @ts-ignore
            { pattern: 123 },
            { name: "test" }
          )
        ).rejects.toThrow("Invalid cron pattern");
        await expect(
          scheduler.upsertJobScheduler(
            schedulerId,
            { every: -100 },
            { name: "test" }
          )
        ).rejects.toThrow("Invalid 'every' value");
        await expect(
          scheduler.upsertJobScheduler(
            schedulerId,
            { pattern: "invalid cron pattern" },
            { name: "test" }
          )
        ).rejects.toThrow(expect.stringContaining("Invalid cron pattern"));
      });

      it("should throw error for empty scheduler ID", async () => {
        await expect(
          scheduler.upsertJobScheduler("", { every: 1000 }, { name: "test" })
        ).rejects.toThrow("Scheduler ID cannot be empty");
      });

      it("should handle Redis errors during upsert", async () => {
        const schedulerId = "redis-fail-upsert";
        const error = new Error("Redis unavailable");
        execSpy.mockRejectedValueOnce(error); // Make exec fail

        await expect(
          scheduler.upsertJobScheduler(
            schedulerId,
            { every: 1000 },
            { name: "fail" }
          )
        ).rejects.toThrow(error);
      });
    });

    describe("removeJobScheduler", () => {
      it("should remove an existing job scheduler", async () => {
        const schedulerId = "remove-me";
        // Assume it exists (we don't need to actually add it due to mocks)

        execSpy.mockResolvedValueOnce([
          [null, 1],
          [null, 1],
        ]); // Simulate DEL=1, ZREM=1

        const result = await scheduler.removeJobScheduler(schedulerId);

        expect(result).toBe(true);
        const expectedKey = `testprefix:${queue.name}:schedulers:${schedulerId}`;

        expect(multiSpy).toHaveBeenCalledTimes(1);
        const pipelineInstance = multiSpy.mock.results[0].value;
        expect(pipelineInstance.del).toHaveBeenCalledWith(expectedKey);
        expect(pipelineInstance.zrem).toHaveBeenCalledWith(
          scheduler["keys"].index,
          schedulerId
        );
        expect(execSpy).toHaveBeenCalledTimes(1);
      });

      it("should return false if scheduler ID does not exist", async () => {
        const schedulerId = "does-not-exist";
        execSpy.mockResolvedValueOnce([
          [null, 0],
          [null, 0],
        ]); // Simulate DEL=0, ZREM=0

        const result = await scheduler.removeJobScheduler(schedulerId);

        expect(result).toBe(false);
        expect(multiSpy).toHaveBeenCalledTimes(1);
        expect(execSpy).toHaveBeenCalledTimes(1);
      });

      it("should return false for empty scheduler ID", async () => {
        const result = await scheduler.removeJobScheduler("");
        expect(result).toBe(false);
        expect(multiSpy).not.toHaveBeenCalled();
      });

      it("should handle Redis errors during remove", async () => {
        const schedulerId = "redis-fail-remove";
        const error = new Error("Redis unavailable");
        execSpy.mockRejectedValueOnce(error); // Make exec fail

        await expect(scheduler.removeJobScheduler(schedulerId)).rejects.toThrow(
          error
        );
      });

      it("should prevent removal if scheduler is closing", async () => {
        const schedulerId = "remove-while-closing";
        const closePromise = scheduler.close(); // Start closing
        await delay(5); // Give closing a moment to start

        await expect(scheduler.removeJobScheduler(schedulerId)).resolves.toBe(
          false
        ); // Should not throw but return false or warn

        await closePromise; // Wait for close to complete
      });
    });

    describe("start/stop/close", () => {
      it("should start the scheduler and schedule the first check", async () => {
        const setTimeoutSpy = spyOn(global, "setTimeout").mockImplementation(
          () => {}
        );

        scheduler.start();
        // @ts-ignore
        expect(scheduler.running).toBe(true);
        expect(setTimeoutSpy).toHaveBeenCalledTimes(1);
        // @ts-ignore
        expect(setTimeoutSpy).toHaveBeenCalledWith(
          expect.any(Function),
          scheduler.opts.checkInterval
        );

        setTimeoutSpy.mockRestore();
      });

      it("should stop the scheduler and clear the timer", async () => {
        const clearTimeoutSpy = spyOn(global, "clearTimeout");
        const fakeTimer = setTimeout(() => {}, 10000);
        // @ts-ignore - Set manually for test
        scheduler.checkTimer = fakeTimer;
        // @ts-ignore - Set manually for test
        scheduler.running = true;

        scheduler.stop();
        // @ts-ignore
        expect(scheduler.running).toBe(false);
        expect(clearTimeoutSpy).toHaveBeenCalledWith(fakeTimer);
        // @ts-ignore
        expect(scheduler.checkTimer).toBeNull();

        clearTimeoutSpy.mockRestore();
      });

      it("should close the scheduler, stop it, and clear cron cache", async () => {
        const stopSpy = spyOn(scheduler, "stop");
        const cronClearSpy = spyOn(scheduler["cronCache"], "clear"); // Access private cache

        // @ts-ignore - Set manually for test
        scheduler.running = true;
        // Add dummy entry to cache
        scheduler["cronCache"].set("dummy", {} as any);

        await scheduler.close();

        expect(stopSpy).toHaveBeenCalledTimes(1);
        expect(cronClearSpy).toHaveBeenCalledTimes(1);
        // @ts-ignore
        expect(scheduler.closing).toBeInstanceOf(Promise); // Should be resolving/resolved
        // @ts-ignore
        expect(scheduler.running).toBe(false); // Should be stopped by close
      });

      it("should prevent starting/stopping if closing", async () => {
        const closePromise = scheduler.close();
        await delay(5); // Allow close to start

        scheduler.start(); // Should have no effect
        // @ts-ignore
        expect(scheduler.running).toBe(false);

        scheduler.stop(); // Should have no effect

        await closePromise;
      });
    });

    describe("_checkAndProcessDueJobs / _processSingleScheduler", () => {
      let queueAddSpy: any;

      beforeEach(() => {
        // Mock queue.add for these tests
        queueAddSpy = spyOn(queue, "add").mockResolvedValue({} as Job); // Simulate successful job add
        // Mock setTimeout/clearTimeout for precise control
        spyOn(global, "setTimeout").mockImplementation((fn) => {
          // Immediately call the function in tests for simplicity,
          // unless specific timing tests are needed
          // fn(); // Or return a dummy timer ID if clear is tested
          return 12345 as any; // Return dummy timer ID
        });
        spyOn(global, "clearTimeout");
      });

      afterEach(() => {
        mock.restore(); // Restore setTimeout/clearTimeout
      });

      it("should find due jobs and process them", async () => {
        const now = Date.now();
        (Date.now as any).mockReturnValue(now);
        const schedulerId1 = "due-job-1";
        const schedulerId2 = "due-job-2";
        const jobTemplate1: JobTemplate = { name: "task1", data: { i: 1 } };
        const jobTemplate2: JobTemplate = {
          name: "task2",
          data: { i: 2 },
          opts: { attempts: 5 },
        };
        const repeat1: SchedulerRepeatOptions = { every: 5000 }; // Next run = now + 5000
        const repeat2: SchedulerRepeatOptions = { pattern: "* * * * *" }; // Next run calculated by Croner
        const nextRun2 = new Cron(repeat2.pattern!)
          .nextRun(new Date(now))!
          .getTime();

        // Mock Redis responses
        zrangebyscoreSpy.mockResolvedValueOnce([schedulerId1, schedulerId2]);
        hgetallSpy
          .mockResolvedValueOnce({
            // Data for schedulerId1
            id: schedulerId1,
            type: "every",
            value: "5000",
            nextRun: (now - 100).toString(), // Due
            name: jobTemplate1.name,
            data: JSON.stringify(jobTemplate1.data),
            opts: "{}",
          })
          .mockResolvedValueOnce({
            // Data for schedulerId2
            id: schedulerId2,
            type: "cron",
            value: repeat2.pattern!,
            nextRun: (now - 50).toString(), // Due
            name: jobTemplate2.name,
            data: JSON.stringify(jobTemplate2.data),
            opts: JSON.stringify(jobTemplate2.opts),
          });

        // Mock multi/exec for state updates
        execSpy.mockResolvedValue([[null, 1]]); // Simulate success for both updates

        // @ts-ignore - Manually trigger the check
        await scheduler._checkAndProcessDueJobs();

        // --- Assertions for Job 1 (every) ---
        expect(queueAddSpy).toHaveBeenCalledWith(
          jobTemplate1.name,
          jobTemplate1.data,
          expect.objectContaining({
            // Default queue opts + scheduler opts + template opts
            // Explicitly check that jobId and delay are undefined
            jobId: undefined,
            delay: undefined,
            // Other options might be merged, check core ones
          })
        );
        // Check Redis update for Job 1
        const expectedKey1 = `testprefix:${queue.name}:schedulers:${schedulerId1}`;
        const expectedNextRun1 = now + repeat1.every!;
        // Ensure multi/exec was called twice (once per job)
        expect(multiSpy).toHaveBeenCalledTimes(2);
        expect(execSpy).toHaveBeenCalledTimes(2);

        // Check the HSET and ZADD calls within the first multi/exec
        const pipelineInstance1 = multiSpy.mock.results[0].value;
        expect(pipelineInstance1.hset).toHaveBeenCalledWith(
          expectedKey1,
          "nextRun",
          expectedNextRun1.toString(),
          "lastRun",
          now.toString()
        );
        expect(pipelineInstance1.zadd).toHaveBeenCalledWith(
          scheduler["keys"].index,
          expectedNextRun1,
          schedulerId1
        );

        // --- Assertions for Job 2 (cron) ---
        expect(queueAddSpy).toHaveBeenCalledWith(
          jobTemplate2.name,
          jobTemplate2.data,
          expect.objectContaining({
            attempts: 5, // From template opts
            jobId: undefined,
            delay: undefined,
          })
        );
        // Check Redis update for Job 2
        const expectedKey2 = `testprefix:${queue.name}:schedulers:${schedulerId2}`;
        const pipelineInstance2 = multiSpy.mock.results[1].value;
        expect(pipelineInstance2.hset).toHaveBeenCalledWith(
          expectedKey2,
          "nextRun",
          nextRun2.toString(),
          "lastRun",
          now.toString()
        );
        expect(pipelineInstance2.zadd).toHaveBeenCalledWith(
          scheduler["keys"].index,
          nextRun2,
          schedulerId2
        );
      });

      it("should skip processing if no jobs are due", async () => {
        zrangebyscoreSpy.mockResolvedValueOnce([]); // No jobs due

        // @ts-ignore
        await scheduler._checkAndProcessDueJobs();

        expect(hgetallSpy).not.toHaveBeenCalled();
        expect(queueAddSpy).not.toHaveBeenCalled();
        expect(multiSpy).not.toHaveBeenCalled();
      });

      it("should skip processing if scheduler is stopped/closing mid-loop", async () => {
        const now = Date.now();
        (Date.now as any).mockReturnValue(now);
        const schedulerId1 = "due-job-1";
        const schedulerId2 = "due-job-2"; // This one won't be processed

        zrangebyscoreSpy.mockResolvedValueOnce([schedulerId1, schedulerId2]);
        hgetallSpy.mockResolvedValueOnce({
          // Data for schedulerId1
          id: schedulerId1,
          type: "every",
          value: "5000",
          nextRun: (now - 100).toString(),
          name: "task1",
          data: "{}",
          opts: "{}",
        });
        // Will not call hgetall for the second job

        // Mock scheduler stopping after the first job is processed
        const processSpy = spyOn(
          scheduler as any,
          "_processSingleScheduler"
        ).mockImplementation(async (id) => {
          if (id === schedulerId1) {
            // Process first job normally (calls mocked add, multi, exec)
            await JobScheduler.prototype["_processSingleScheduler"].call(
              scheduler,
              id,
              now
            ); // Call original logic
            // Now simulate stop
            scheduler.stop(); // Stop the scheduler
          } else {
            // Should not be called for id2
            throw new Error("Should not process second job");
          }
        });

        // @ts-ignore
        await scheduler._checkAndProcessDueJobs();

        expect(processSpy).toHaveBeenCalledTimes(1); // Only called for the first job
        expect(processSpy).toHaveBeenCalledWith(schedulerId1, now);
        expect(queueAddSpy).toHaveBeenCalledTimes(1); // Add called for first job
        expect(multiSpy).toHaveBeenCalledTimes(1); // Multi called for first job update

        processSpy.mockRestore(); // Restore original method
      });

      it("should handle missing scheduler data after zrangebyscore (race condition)", async () => {
        const schedulerId = "deleted-job";
        zrangebyscoreSpy.mockResolvedValueOnce([schedulerId]);
        hgetallSpy.mockResolvedValueOnce({}); // Simulate HGETALL returning empty

        // Expect ZREM to be called for cleanup
        zremSpy.mockClear(); // Clear previous calls if any

        // @ts-ignore
        await scheduler._checkAndProcessDueJobs();

        expect(hgetallSpy).toHaveBeenCalledWith(
          `testprefix:${queue.name}:schedulers:${schedulerId}`
        );
        expect(zremSpy).toHaveBeenCalledWith(
          scheduler["keys"].index,
          schedulerId
        );
        expect(queueAddSpy).not.toHaveBeenCalled();
        expect(multiSpy).not.toHaveBeenCalled(); // No update attempts
      });

      it("should handle job add failure and attempt recovery", async () => {
        const now = Date.now();
        (Date.now as any).mockReturnValue(now);
        const schedulerId = "fail-add-job";
        const jobTemplate: JobTemplate = { name: "wont-add" };
        const addError = new Error("Queue is full");
        const checkInterval = scheduler["opts"].checkInterval;
        const recoveryNextRun = now + checkInterval; // Expected recovery time

        zrangebyscoreSpy.mockResolvedValueOnce([schedulerId]);
        hgetallSpy.mockResolvedValueOnce({
          id: schedulerId,
          type: "every",
          value: "5000",
          nextRun: (now - 100).toString(),
          name: jobTemplate.name,
          data: "{}",
          opts: "{}",
        });
        queueAddSpy.mockRejectedValueOnce(addError); // Make queue.add fail

        // Mock multi/exec for the recovery attempt
        multiSpy.mockClear();
        execSpy.mockClear();
        execSpy.mockResolvedValueOnce([[null, 1]]); // Recovery update succeeds

        // @ts-ignore
        await scheduler._checkAndProcessDueJobs();

        expect(queueAddSpy).toHaveBeenCalledTimes(1);
        expect(multiSpy).toHaveBeenCalledTimes(1); // Only the recovery multi call

        // Check the recovery HSET and ZADD
        const pipelineInstance = multiSpy.mock.results[0].value;
        const expectedKey = `testprefix:${queue.name}:schedulers:${schedulerId}`;
        expect(pipelineInstance.hset).toHaveBeenCalledWith(
          expectedKey,
          "nextRun",
          recoveryNextRun.toString()
        );
        expect(pipelineInstance.zadd).toHaveBeenCalledWith(
          scheduler["keys"].index,
          recoveryNextRun,
          schedulerId
        );
        expect(execSpy).toHaveBeenCalledTimes(1);
      });

      it("should handle recovery failure by removing from index", async () => {
        const now = Date.now();
        (Date.now as any).mockReturnValue(now);
        const schedulerId = "fail-recovery";
        const jobTemplate: JobTemplate = { name: "wont-recover" };
        const addError = new Error("Queue failed");
        const recoveryError = new Error("Redis recovery failed");

        zrangebyscoreSpy.mockResolvedValueOnce([schedulerId]);
        hgetallSpy.mockResolvedValueOnce({
          id: schedulerId,
          type: "every",
          value: "5000",
          nextRun: (now - 100).toString(),
          name: jobTemplate.name,
          data: "{}",
          opts: "{}",
        });
        queueAddSpy.mockRejectedValueOnce(addError);

        // Mock multi/exec to fail for the recovery attempt
        multiSpy.mockClear();
        execSpy.mockClear();
        execSpy.mockRejectedValueOnce(recoveryError);

        // Mock the final ZREM call
        zremSpy.mockClear();
        zremSpy.mockResolvedValueOnce(1);

        const consoleErrorSpy = spyOn(console, "error");

        // @ts-ignore
        await scheduler._checkAndProcessDueJobs();

        expect(queueAddSpy).toHaveBeenCalledTimes(1);
        expect(multiSpy).toHaveBeenCalledTimes(1); // Recovery multi was called
        expect(execSpy).toHaveBeenCalledTimes(1); // Recovery exec failed
        expect(zremSpy).toHaveBeenCalledWith(
          scheduler["keys"].index,
          schedulerId
        ); // Cleanup ZREM
        expect(consoleErrorSpy).toHaveBeenCalledWith(
          expect.stringContaining(`Removed scheduler ${schedulerId} from index`)
        );

        consoleErrorSpy.mockRestore();
      });

      it("should skip job if its nextRun is in the future (race condition handled by another process)", async () => {
        const now = Date.now();
        (Date.now as any).mockReturnValue(now);
        const schedulerId = "future-job";
        const futureNextRun = now + 10000; // 10 seconds in the future

        zrangebyscoreSpy.mockResolvedValueOnce([schedulerId]); // Found by score <= now
        hgetallSpy.mockResolvedValueOnce({
          // But HGETALL reveals it was updated
          id: schedulerId,
          type: "every",
          value: "5000",
          nextRun: futureNextRun.toString(),
          name: "futureTask",
          data: "{}",
          opts: "{}",
        });

        // Mock zadd for the corrective update
        zaddSpy.mockClear();
        zaddSpy.mockResolvedValueOnce(1);

        // @ts-ignore
        await scheduler._checkAndProcessDueJobs();

        expect(hgetallSpy).toHaveBeenCalledTimes(1);
        // Corrective ZADD with NX should be called
        expect(zaddSpy).toHaveBeenCalledWith(
          scheduler["keys"].index,
          "NX", // Ensure NX flag is used
          futureNextRun,
          schedulerId
        );
        expect(queueAddSpy).not.toHaveBeenCalled(); // Should not add the job
        expect(multiSpy).not.toHaveBeenCalled(); // Should not try to update state via multi
      });
    });

    describe("parseSchedulerData", () => {
      it("should parse valid cron data", () => {
        const hash = {
          id: "s1",
          type: "cron",
          value: "* * * * *",
          tz: "UTC",
          nextRun: "1700000000000",
          name: "j1",
          data: '{"a":1}',
          opts: '{"b":2}',
          lastRun: "1699999940000",
        };
        // @ts-ignore Access private method
        const result = scheduler.parseSchedulerData(hash);
        expect(result).toEqual({
          id: "s1",
          type: "cron",
          value: "* * * * *",
          tz: "UTC",
          nextRun: 1700000000000,
          name: "j1",
          data: { a: 1 },
          // @ts-ignore
          opts: { b: 2 },
          lastRun: 1699999940000,
        });
      });

      it("should parse valid every data", () => {
        const hash = {
          id: "s2",
          type: "every",
          value: "5000",
          nextRun: "1700000005000",
          name: "j2",
          data: "[]",
          opts: "{}",
          // tz and lastRun are optional
        };
        // @ts-ignore
        const result = scheduler.parseSchedulerData(hash);
        expect(result).toEqual({
          id: "s2",
          type: "every",
          value: 5000,
          tz: undefined,
          nextRun: 1700000005000,
          name: "j2",
          data: [],
          opts: {},
          lastRun: undefined,
        });
      });

      it("should return null for invalid JSON", () => {
        const hash = {
          id: "s3",
          type: "cron",
          value: "* * * * *",
          nextRun: "1700000000000",
          name: "j3",
          data: "{invalid",
          opts: "{}",
        };
        // @ts-ignore
        expect(scheduler.parseSchedulerData(hash)).toBeNull();
      });

      it("should return null for missing required fields", () => {
        const hash = {
          /* missing id */ type: "cron",
          value: "* * * * *",
          nextRun: "1700000000000",
          name: "j4",
        };
        // @ts-ignore
        expect(scheduler.parseSchedulerData(hash)).toBeNull();
      });

      it("should return null for invalid type", () => {
        const hash = {
          id: "s5",
          type: "wrong",
          value: "5000",
          nextRun: "1700000000000",
          name: "j5",
        };
        // @ts-ignore
        expect(scheduler.parseSchedulerData(hash)).toBeNull();
      });
    });

    describe("parseCron", () => {
      it("should parse cron string and cache the result", () => {
        const pattern = "0 * * * *";
        const now = Date.now();

        // Mock Cron constructor and nextRun
        const mockCronInstance = {
          nextRun: mock().mockReturnValue(new Date(now + 3600000)),
        };
        const CronMock = mock().mockImplementation(() => mockCronInstance);
        mock.module("croner", () => ({ Cron: CronMock })); // Doesn't work with bun mock

        // --- Direct spy instead of module mock ---
        const nextRunSpy = spyOn(Cron.prototype, "nextRun").mockReturnValue(
          new Date(now + 3600000)
        );
        const constructorSpy = spyOn(Cron.prototype, "constructor"); // Less reliable way to check constructor calls

        // @ts-ignore Access private method & cache
        const cache = scheduler.cronCache;
        expect(cache.size).toBe(0);

        // First call (miss)
        // @ts-ignore
        const result1 = scheduler.parseCron(pattern, now);
        expect(result1).toBeInstanceOf(Cron);
        // expect(constructorSpy).toHaveBeenCalledTimes(1); // Check constructor call (might be unreliable)
        expect(cache.size).toBe(1);
        expect(cache.has(`${pattern}_local`)).toBe(true);

        // Second call (hit)
        // @ts-ignore
        const result2 = scheduler.parseCron(pattern, now);
        expect(result2).toBe(result1); // Should return cached instance
        //  expect(constructorSpy).toHaveBeenCalledTimes(1); // Constructor should NOT be called again
        expect(cache.size).toBe(1);

        nextRunSpy.mockRestore();
      });

      it("should handle timezone correctly", () => {
        const pattern = "0 9 * * 1-5"; // 9 AM on weekdays
        const tz = "America/New_York";
        const now = Date.now();

        // Use the real Croner logic here, just verify it's called with TZ
        const CronMock = mock();
        // mock.module('croner', () => ({ Cron: CronMock })); // Doesn't work with bun mock

        const constructorSpy = spyOn(Cron.prototype, "constructor");

        // @ts-ignore Access private method
        scheduler.parseCron(pattern, now, tz);

        // Check if Cron constructor was called with timezone options
        // This is hard to assert directly with spies on prototype.
        // Instead, we rely on the fact that if TZ is passed, Croner handles it.
        // We can check the cache key.
        // @ts-ignore
        expect(scheduler.cronCache.has(`${pattern}_${tz}`)).toBe(true);
      });
    });
  });

  describe("Worker Class", () => {
    it("should process a job successfully", async () => {
      const queue = createQueue<{ input: number }, { output: number }>(
        testQueueName
      );
      const jobData = { input: 5 };
      const expectedResult = { output: 10 };
      let processedJob: Job | null = null;
      let jobResult: any = null;

      const processor: Processor<
        { input: number },
        { output: number }
      > = async (job) => {
        processedJob = job;
        await delay(50); // Simulate work
        return { output: job.data.input * 2 };
      };

      const worker = createWorker(testQueueName, processor);

      const completedPromise = new Promise<void>((resolve) => {
        worker.on("completed", (job, result) => {
          if (job.id === addedJob.id) {
            jobResult = result;
            resolve();
          }
        });
      });

      const addedJob = await queue.add("multiply", jobData);
      await completedPromise; // Wait for the 'completed' event

      expect(processedJob).not.toBeNull();
      expect(processedJob!.id).toBe(addedJob.id);
      expect(processedJob!.data).toEqual(jobData);
      expect(jobResult).toEqual(expectedResult);

      const counts = await queue.getJobCounts();
      expect(counts.completed).toBe(1);
      expect(counts.wait).toBe(0);
      expect(counts.active).toBe(0);

      const retrievedJob = await queue.getJob(addedJob.id);
      expect(retrievedJob).not.toBeNull();
      expect(retrievedJob!.finishedOn).toBeNumber();
      expect(retrievedJob!.returnValue).toEqual(expectedResult);
      expect(retrievedJob!.lockToken).toBeUndefined(); // Lock should be released
    });

    it("should move a job to failed after exhausting retries", async () => {
      const queue = createQueue<{ fail: boolean }>(testQueueName);
      const jobData = { fail: true };
      const maxAttempts = 2;
      const failError = new Error("Job failed as planned");
      let attemptsMade = 0;
      let failedJob: Job | null = null;
      let receivedError: Error | null = null;

      const processor: Processor<{ fail: boolean }> = async (job) => {
        attemptsMade++;
        await delay(20);
        throw failError;
      };

      const worker = createWorker(testQueueName, processor, { concurrency: 1 });

      const failedPromise = new Promise<void>((resolve) => {
        worker.on("failed", (job, err) => {
          if (job?.id === addedJob.id) {
            failedJob = job;
            receivedError = err;
            // Resolve only on the final failure
            if (job?.attemptsMade === maxAttempts) {
              resolve();
            }
          }
        });
      });

      const addedJob = await queue.add("fail-job", jobData, {
        attempts: maxAttempts,
      });

      await failedPromise; // Wait for the final 'failed' event

      expect(attemptsMade).toBe(maxAttempts);
      expect(failedJob).not.toBeNull();
      expect(failedJob!.id).toBe(addedJob.id);
      expect(failedJob!.attemptsMade).toBe(maxAttempts);
      // @ts-ignore
      expect(receivedError).toBe(failError);

      const counts = await queue.getJobCounts();
      expect(counts.failed).toBe(1);
      expect(counts.completed).toBe(0);
      expect(counts.wait).toBe(0);
      expect(counts.active).toBe(0);

      const retrievedJob = await queue.getJob(addedJob.id);
      expect(retrievedJob).not.toBeNull();
      expect(retrievedJob!.failedReason).toBe(failError.message);
      expect(retrievedJob!.attemptsMade).toBe(maxAttempts);
      expect(retrievedJob!.finishedOn).toBeNumber();
      expect(retrievedJob!.stacktrace).toBeArray();
      expect(retrievedJob!.stacktrace!.length).toBeGreaterThan(0);
    });

    it("should handle fixed backoff strategy", async () => {
      const queue = createQueue<{ fail: boolean }>(testQueueName);
      const maxAttempts = 3;
      const backoffDelay = 100; // ms
      let processorCallTimestamps: number[] = [];

      const processor: Processor<{ fail: boolean }> = async (job) => {
        processorCallTimestamps.push(Date.now());
        await delay(10);
        throw new Error("Failing for backoff test");
      };

      const worker = createWorker(testQueueName, processor);

      const failedPromise = new Promise<void>((resolve) => {
        worker.on("failed", (job, err) => {
          if (job?.id === addedJob.id && job?.attemptsMade === maxAttempts) {
            resolve();
          }
        });
      });

      const addedJob = await queue.add(
        "backoff-job",
        { fail: true },
        {
          attempts: maxAttempts,
          backoff: { type: "fixed", delay: backoffDelay },
        }
      );

      await failedPromise;

      expect(processorCallTimestamps.length).toBe(maxAttempts);

      // Check approximate delay between attempts
      for (let i = 1; i < maxAttempts; i++) {
        const diff =
          processorCallTimestamps[i]! - processorCallTimestamps[i - 1]!;
        // Allow significant tolerance for test runner / event loop delays
        expect(diff).toBeGreaterThanOrEqual(backoffDelay - 20); // Lower bound
        expect(diff).toBeLessThan(backoffDelay + 500); // Upper bound (generous)
      }

      const counts = await queue.getJobCounts();
      expect(counts.failed).toBe(1);
    });

    it("should handle exponential backoff strategy", async () => {
      const queue = createQueue<{ fail: boolean }>(testQueueName);
      const maxAttempts = 3; // Will have delays of 100, 200
      const initialDelay = 100; // ms
      let processorCallTimestamps: number[] = [];

      const processor: Processor<{ fail: boolean }> = async (job) => {
        processorCallTimestamps.push(Date.now());
        await delay(10);
        throw new Error("Failing for exponential backoff test");
      };

      const worker = createWorker(testQueueName, processor);

      const failedPromise = new Promise<void>((resolve) => {
        worker.on("failed", (job, err) => {
          if (job?.id === addedJob.id && job?.attemptsMade === maxAttempts) {
            resolve();
          }
        });
      });

      const addedJob = await queue.add(
        "exp-backoff-job",
        { fail: true },
        {
          attempts: maxAttempts,
          backoff: { type: "exponential", delay: initialDelay },
        }
      );

      await failedPromise;

      expect(processorCallTimestamps.length).toBe(maxAttempts);

      // Check approximate delays: ~100ms, ~200ms
      const diff1 = processorCallTimestamps[1]! - processorCallTimestamps[0]!;
      const diff2 = processorCallTimestamps[2]! - processorCallTimestamps[1]!;

      expect(diff1).toBeGreaterThanOrEqual(initialDelay - 20);
      expect(diff1).toBeLessThan(initialDelay + 500);

      const expectedSecondDelay = initialDelay * 2;
      expect(diff2).toBeGreaterThanOrEqual(expectedSecondDelay - 40); // Allow more variation
      expect(diff2).toBeLessThan(expectedSecondDelay + 1100); // Generous upper bound

      const counts = await queue.getJobCounts();
      expect(counts.failed).toBe(1);
    });

    it("should respect concurrency limits", async () => {
      const queue = createQueue<{ index: number }>(testQueueName);
      const concurrency = 2;
      const jobCount = 5;
      const jobProcessTime = 200;
      let maxConcurrent = 0;
      let currentConcurrent = 0;
      let completedCount = 0;

      const processor: Processor<{ index: number }, boolean> = async (job) => {
        currentConcurrent++;
        maxConcurrent = Math.max(maxConcurrent, currentConcurrent);
        await delay(jobProcessTime);
        currentConcurrent--;
        return true;
      };

      const worker = createWorker(testQueueName, processor, { concurrency });

      const completionPromises = new Array(jobCount).fill(null).map(
        () =>
          new Promise<void>((resolve) => {
            worker.on("completed", (job, result) => {
              // Resolve specific promise based on job ID - needs mapping
              // Simpler: just count completed jobs
              completedCount++;
              if (completedCount === jobCount) {
                // This might resolve multiple times, handle outside
              }
              resolve(); // Resolve on any completion for simplicity now
            });
          })
      );

      const allJobsCompleted = new Promise<void>((res) => {
        const interval = setInterval(async () => {
          const counts = await queue.getJobCounts();
          if (counts.completed === jobCount) {
            clearInterval(interval);
            res();
          }
        }, 50);
      });

      for (let i = 0; i < jobCount; i++) {
        await queue.add("concurrent-job", { index: i });
      }

      await waitFor(
        async () => (await queue.getJobCounts()).completed === jobCount,
        5000
      );
      await allJobsCompleted; // Wait for all jobs to complete

      expect(maxConcurrent).toBe(concurrency);

      const finalCounts = await queue.getJobCounts();
      expect(finalCounts.completed).toBe(jobCount);
      expect(finalCounts.active).toBe(0);
      expect(finalCounts.wait).toBe(0);
    });

    it("should remove job data on complete if specified", async () => {
      const queue = createQueue(testQueueName);
      const worker = createWorker(
        testQueueName,
        async (job) => {
          await delay(10);
          return "ok";
        },
        {
          removeOnComplete: true,
        }
      );
      const job = await queue.add("complete-remove", { d: 1 });

      await waitFor(async () => (await queue.getJob(job.id)) === null, 2000);

      // Wait a tiny bit more for potential DEL command
      await delay(100);

      const retrievedJob = await queue.getJob(job.id);
      expect(retrievedJob).toBeNull(); // Job data should be deleted

      const counts = await queue.getJobCounts();
      expect(counts.completed).toBe(0); // Count also depends on script logic for ZSET removal
      // Verify the moveToCompleted script removed from ZSET if removeOnComplete=true
      // This requires checking ZCARD or ZSCORE, assuming script DELs job *and* doesn't ZADD
      const completedSetSize = await redisClient.zcard(queue.keys.completed);
      expect(completedSetSize).toBe(0);
    });

    it("should remove job data on fail if specified", async () => {
      const queue = createQueue(testQueueName);
      const worker = createWorker(
        testQueueName,
        async (job) => {
          await delay(10);
          throw new Error("fail");
        },
        {
          removeOnFail: true,
          concurrency: 1,
        }
      );
      const job = await queue.add("fail-remove", { d: 1 }, { attempts: 1 }); // Only 1 attempt

      await waitFor(async () => (await queue.getJob(job.id)) === null, 2000);

      await delay(100); // Wait for potential DEL command

      const retrievedJob = await queue.getJob(job.id);
      expect(retrievedJob).toBeNull(); // Job data should be deleted

      // Verify the moveToFailed script removed from ZSET if removeOnFail=true
      const failedSetSize = await redisClient.zcard(queue.keys.failed);
      expect(failedSetSize).toBe(0);
    });

    it("should keep a specified number of completed jobs", async () => {
      const keepCount = 2;
      const jobCount = 5;
      const queue = createQueue(testQueueName);
      const worker = createWorker(
        testQueueName,
        async (job) => {
          await delay(5);
          return job.data.i;
        },
        {
          removeOnComplete: keepCount, // Keep only 2 completed jobs
        }
      );

      let completedJobs = 0;
      const completedPromise = new Promise<void>((res) => {
        worker.on("completed", () => {
          completedJobs++;
          if (completedJobs === jobCount) res();
        });
      });

      const addedJobs: Job[] = [];
      for (let i = 0; i < jobCount; i++) {
        addedJobs.push(await queue.add("complete-keep", { i }));
      }

      await waitFor(() => completedJobs === jobCount, 5000);
      await completedPromise; // Ensure processor finished

      const counts = await queue.getJobCounts();
      expect(counts.completed).toBe(keepCount);

      // Verify job data still exists for all (since Lua doesn't DEL based on count)
      for (const job of addedJobs) {
        const retrieved = await queue.getJob(job.id);
        expect(retrieved).not.toBeNull();
        expect(retrieved!.returnValue).toBeNumber();
      }
    });

    it("should close gracefully, waiting for active jobs", async () => {
      const queue = createQueue(testQueueName);
      const jobProcessTime = 300;
      let jobStarted = false;
      let jobFinished = false;

      const worker = createWorker(
        testQueueName,
        async (job) => {
          jobStarted = true;
          await delay(jobProcessTime);
          jobFinished = true;
          return "done";
        },
        { concurrency: 1 }
      );

      await queue.add("slow-job", { d: 1 });

      // Wait for the job to become active
      await waitFor(
        async () => (await queue.getJobCounts()).active === 1,
        1000
      );
      expect(jobStarted).toBe(true);

      // Initiate close *while* the job is running
      const closePromise = worker.close(); // Don't force close

      // Check that the job hasn't finished immediately
      await delay(jobProcessTime / 2);
      expect(jobFinished).toBe(false);

      // Wait for the close promise to resolve
      await closePromise;

      // Check that the job completed before close resolved
      expect(jobFinished).toBe(true);

      const counts = await queue.getJobCounts();
      expect(counts.completed).toBe(1); // Job should complete
      expect(counts.active).toBe(0);

      // Worker should be removed from cleanup list as it's closed
      workersToClose = workersToClose.filter((w) => w !== worker);
    });

    it("should emit an error event for processor errors not caught by try/catch", async () => {
      // This tests if the main loop's catch handles unexpected errors
      const queue = createQueue(testQueueName);
      const errorMessage = "Critical processor failure";
      let caughtError: Error | null = null;
      let failedJob: Job | null = null;

      const processor = (job: Job) => {
        throw new Error(errorMessage);
      };

      const worker = createWorker(testQueueName, processor as any, {
        concurrency: 1,
      });

      const failedPromise = new Promise<void>((resolve) => {
        worker.on("failed", (job, err) => {
          caughtError = err;
          failedJob = job;
          resolve();
        });
        worker.on("error", (err) => {
          console.error(
            "Worker 'error' event fired unexpectedly in test:",
            err
          );
        });
      });

      const addedJob = await queue.add("error-job", { d: 1 }, { attempts: 1 });

      await failedPromise;

      expect(failedJob).not.toBeNull();
      expect(caughtError).toBeInstanceOf(Error);
      expect(caughtError!.message).toBe(errorMessage);

      // Job might end up in failed state depending on error handling details
      await delay(100);
      const counts = await queue.getJobCounts();
      expect(counts.failed).toBe(1);
      const retrievedJob = await queue.getJob(addedJob.id);
      expect(retrievedJob).not.toBeNull();
      expect(retrievedJob?.failedReason).toBe(errorMessage);
    });

    it("should emit 'active' event when processing starts", async () => {
      const queue = createQueue(testQueueName);
      let isActiveEventEmitted = false;
      let activeJob: Job | null = null;

      const worker = createWorker(testQueueName, async (job) => {
        await delay(50);
        return "ok";
      });

      worker.on("active", (job) => {
        isActiveEventEmitted = true;
        activeJob = job;
      });

      const addedJob = await queue.add("active-event-job", { d: 1 });

      await waitFor(() => isActiveEventEmitted, 2000);

      expect(isActiveEventEmitted).toBe(true);
      expect(activeJob).not.toBeNull();
      expect(activeJob!.id).toBe(addedJob.id);
    });

    it("should emit 'retrying' event when a job is retried", async () => {
      const queue = createQueue(testQueueName);
      const maxAttempts = 2;
      const failError = new Error("Temporary failure");
      let retryEventEmitted = false;
      let retryingJob: Job | null = null;
      let retryError: Error | null = null;

      const processor: Processor = async (job) => {
        await delay(10);
        throw failError;
      };

      const worker = createWorker(testQueueName, processor, { concurrency: 1 });

      worker.on("retrying", (job, err) => {
        retryEventEmitted = true;
        retryingJob = job;
        retryError = err;
      });

      const failedPromise = new Promise<void>((resolve) => {
        worker.on("failed", (job, err) => {
          // Wait for final failure
          if (job?.id === addedJob.id && job?.attemptsMade === maxAttempts) {
            resolve();
          }
        });
      });

      const addedJob = await queue.add(
        "retry-event-job",
        { d: 1 },
        { attempts: maxAttempts, backoff: 10 }
      );

      await waitFor(() => retryEventEmitted, 2000); // Wait for the retry event specifically

      expect(retryEventEmitted).toBe(true);
      expect(retryingJob).not.toBeNull();
      expect(retryingJob!.id).toBe(addedJob.id);
      expect(retryingJob!.attemptsMade).toBe(1); // AttemptsMade *before* retry logic increments it for the next attempt
      // @ts-ignore Try to fix this type error
      expect(retryError).toBe(failError);

      await failedPromise; // Ensure the job eventually fails completely
    });

    it("should emit 'movedDelayed' event when delayed jobs are moved", async () => {
      const queue = createQueue(testQueueName);
      let movedDelayedEventCount = 0;

      // Add a delayed job
      await queue.add("delayed-job-event", { d: 1 }, { delay: 100 });

      const worker = createWorker(
        testQueueName,
        async (job) => {
          await delay(10);
          return "ok";
        },
        { lockDuration: 10000 }
      ); // Longer lock to avoid interference

      worker.on("movedDelayed", (count) => {
        movedDelayedEventCount = count;
      });

      // Wait for the job to be processed, which implies the delayed job was moved
      await waitFor(
        async () => (await queue.getJobCounts()).completed === 1,
        3000
      );

      // Check if the event fired. This might be slightly racy depending on timing.
      // The check happens periodically in the worker loop.
      expect(movedDelayedEventCount).toBeGreaterThanOrEqual(1);
    });

    it("should stop processing *updates* and warn if lock renewal fails", async () => {
      const queue = createQueue(testQueueName);
      const lockDuration = 100; // Very short lock
      const lockRenewTime = 50; // Renew frequently
      const processTime = 300; // Longer than lock duration
      let processorFinishedExecuting = false; // Track if the processor function ran fully
      let jobFailedEvent = false;
      let jobCompletedEvent = false;
      let lockWarningLogged = false;

      const consoleWarnSpy = spyOn(console, "warn"); // Spy on console.warn

      const worker = createWorker(
        testQueueName,
        async (job) => {
          // Simulate changing the lock token externally mid-process
          await delay(lockRenewTime + 20); // Wait until after the first renewal *should* have happened
          await redisClient.hset(
            `${queue.keys.jobs}:${job.id}`,
            "lockToken",
            "invalid-token"
          );
          // Continue processing
          await delay(processTime);

          processorFinishedExecuting = true; // This indicates the async function completed
          return "ok";
        },
        {
          concurrency: 1,
          lockDuration: lockDuration,
          lockRenewTime: lockRenewTime,
        }
      );

      worker.on("completed", () => (jobCompletedEvent = true)); // Shouldn't fire
      worker.on("failed", () => (jobFailedEvent = true)); // Shouldn't fire
      // We expect a console warning for the renewal failure
      consoleWarnSpy.mockImplementation((message: string, ...args) => {
        // Keep original console.warn functionality if needed for debugging
        // console.log('[SPY console.warn]:', message, ...args);
        if (
          typeof message === "string" &&
          message.includes("Failed to renew lock for job")
        ) {
          lockWarningLogged = true;
        }
      });

      const job = await queue.add("lock-renewal-fail", { d: 1 });

      // Wait long enough for the processor to finish *and* for cleanup logic to run
      await delay(processTime + lockDuration + lockRenewTime + 200); // Added extra buffer

      expect(lockWarningLogged).toBe(true); // Check if the renewal failure warning was logged
      expect(processorFinishedExecuting).toBe(true); // The processor *did* finish executing
      expect(jobCompletedEvent).toBe(false); // Job should NOT emit 'completed' event
      expect(jobFailedEvent).toBe(false); // Job should NOT emit 'failed' event

      // Check internal worker state: Job should be removed from jobsInFlight
      // Use waitFor as internal state updates might have slight delays
      await waitFor(() => !(worker as any).jobsInFlight.has(job.id), 1000, 50);
      expect((worker as any).jobsInFlight.has(job.id)).toBe(false);

      // Check job state in Redis - it should NOT have a return value or finishedOn
      // and should not be in the 'completed' list
      const finalJobState = await queue.getJob(job.id);
      expect(finalJobState).not.toBeNull();
      expect(finalJobState!.returnValue).toBeUndefined();
      expect(finalJobState!.finishedOn).toBeUndefined();

      const finalCounts = await queue.getJobCounts();
      expect(finalCounts.completed).toBe(0);

      consoleWarnSpy.mockRestore();
    });

    it("should not complete job if lock token mismatches on completion attempt", async () => {
      const queue = createQueue(testQueueName);
      let jobCompleted = false;
      let jobFailed = false; // It might fail if error handling is robust, or just get stuck

      const worker = createWorker(
        testQueueName,
        async (job) => {
          await delay(50); // Short processing time
          // BEFORE returning, manually change the lock token in Redis
          await redisClient.hset(
            `${queue.keys.jobs}:${job.id}`,
            "lockToken",
            "different-token"
          );
          return "should-not-be-saved";
        },
        { concurrency: 1 }
      );

      worker.on("completed", () => (jobCompleted = true));
      worker.on("failed", () => (jobFailed = true));

      const job = await queue.add("lock-mismatch-complete", { d: 1 });

      // Wait significantly longer than processing time
      await delay(500);

      expect(jobCompleted).toBe(false); // moveToCompleted script should return -1

      // Verify job state in Redis - it shouldn't be in 'completed'
      const finalCounts = await queue.getJobCounts();
      expect(finalCounts.completed).toBe(0);

      // Verify the job data hasn't been updated with the return value
      const finalJobState = await queue.getJob(job.id);
      expect(finalJobState).not.toBeNull();
      expect(finalJobState!.returnValue).toBeUndefined();
      expect(finalJobState!.finishedOn).toBeUndefined();
      // The job might still be in the 'active' list because LREM failed in moveToCompleted script
      expect(finalCounts.active).toBe(1); // Or 0 if some stall detection cleaned it up later
      expect(jobFailed).toBe(false); // It shouldn't fail unless the processor throws an error
    });

    it("should close forcefully, interrupting active jobs", async () => {
      const queue = createQueue(testQueueName);
      const jobProcessTime = 400;
      let jobStarted = false;
      let jobFinished = false;

      const worker = createWorker(
        testQueueName,
        async (job) => {
          jobStarted = true;
          await delay(jobProcessTime); // Simulate long work
          jobFinished = true;
          return "done";
        },
        { concurrency: 1 }
      );

      await queue.add("force-close-job", { d: 1 });

      // Wait for the job to become active
      await waitFor(
        async () => (await queue.getJobCounts()).active === 1,
        1000
      );
      expect(jobStarted).toBe(true);

      // Initiate force close *while* the job is running
      const closeStartTime = Date.now();
      await worker.close(true); // Force close
      const closeEndTime = Date.now();

      // Check that close was fast (didn't wait for job)
      expect(closeEndTime - closeStartTime).toBeLessThan(jobProcessTime);

      // Check that the job did NOT finish
      expect(jobFinished).toBe(false);

      // Job should remain in the active list (or potentially moved later by stall check)
      const counts = await queue.getJobCounts();
      expect(counts.active).toBe(1);
      expect(counts.completed).toBe(0);

      workersToClose = workersToClose.filter((w) => w !== worker); // Already closed
    });
  });

  it("should pause the main loop when concurrency limit is reached", async () => {
    const queue = createQueue(testQueueName);
    const concurrency = 2; // Use concurrency > 1
    const jobCount = 3; // Add more jobs than concurrency
    const jobProcessTime = 250;

    // Spy on the actual delay function from the utils module
    const delaySpy = spyOn(queueUtils, "delay");

    const worker = createWorker(
      testQueueName,
      async (job) => {
        await queueUtils.delay(jobProcessTime); // Use the original delay for processing
        return true;
      },
      { concurrency }
    );

    // Add jobs
    for (let i = 0; i < jobCount; i++) {
      await queue.add("concurrency-pause-job", { index: i });
    }

    // Wait for all jobs to complete
    await waitFor(
      async () => (await queue.getJobCounts()).completed === jobCount,
      jobProcessTime * 2 + 500 // Adjust timeout if needed
    );

    // Check if the specific delay(200) used for pausing was called
    expect(delaySpy).toHaveBeenCalledWith(200);

    // Restore the original function after the test
    delaySpy.mockRestore();

    const finalCounts = await queue.getJobCounts();
    expect(finalCounts.completed).toBe(jobCount);
  });
});
</file>

<file path="README.md">
# LightQ 

[![codecov](https://codecov.io/gh/jlucaso1/lightq/graph/badge.svg)](https://codecov.io/gh/jlucaso1/lightq)
[![NPM Version](https://img.shields.io/npm/v/@jlucaso%2Flightq.svg)](https://www.npmjs.com/package/@jlucaso/lightq)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A simple, lightweight, Redis-backed job queue library designed for TypeScript
applications. LightQ provides essential queueing features with a focus on
reliability and ease of use, leveraging atomic Redis operations via Lua scripts.

## Features

-  **Simple API:** Easy-to-understand interface for adding and processing
  jobs.
-  **Redis Backend:** Uses Redis for persistence, ensuring jobs are not lost.
-  **Atomic Operations:** Leverages Lua scripts for safe, atomic state
  transitions in Redis.
-  **TypeScript Native:** Written entirely in TypeScript with types included.
-  **Delayed Jobs:** Schedule jobs to run after a specific delay.
-  **Automatic Retries:** Failed jobs can be automatically retried with
  configurable backoff strategies (fixed, exponential).
-  **Concurrency Control:** Limit the number of jobs a worker processes
  concurrently.
-  **Job Lifecycle Events:** Emit events for `active`, `completed`, `failed`,
  `retrying`, etc.
-  **Automatic Cleanup:** Optionally remove job data automatically upon
  completion or failure.

## Installation

```bash
# Using npm
npm install @jlucaso/lightq

# Using yarn
yarn add @jlucaso/lightq

# Using Bun
bun add @jlucaso/lightq
```

**Prerequisite:** Requires a running Redis server (version 4.0 or later
recommended due to Lua script usage). `ioredis` is used as the Redis client.

## Basic Usage

Check the ``example/index.ts`

## API Overview

### `Queue<TData, TResult, TName>`

- **`constructor(name: string, opts: QueueOptions)`**: Creates a new queue
  instance.
- **`add(name: TName, data: TData, opts?: JobOptions): Promise<Job<TData, TResult, TName>>`**:
  Adds a single job to the queue.
- **`addBulk(jobs: { name: TName; data: TData; opts?: JobOptions }[]): Promise<Job<TData, TResult, TName>[]>`**:
  Adds multiple jobs efficiently.
- **`getJob(jobId: string): Promise<Job<TData, TResult, TName> | null>`**:
  Retrieves job details by ID.
- **`getJobCounts(): Promise<{ wait: number; active: number; ... }>`**: Gets the
  number of jobs in different states.
- **`close(): Promise<void>`**: Closes the Redis connection used by the queue.
- **Events**: `error`, `ready`, `waiting` (when a job is added/ready),
  `closing`, `closed`.

### `Worker<TData, TResult, TName>`

- **`constructor(name: string, processor: Processor<TData, TResult, TName>, opts: WorkerOptions)`**:
  Creates a worker instance to process jobs from the specified queue.
  - The `processor` function takes a `Job` object and should return a `Promise`
    resolving with the job's result or throwing an error if it fails.
- **`close(force?: boolean): Promise<void>`**: Closes the worker. By default, it
  waits for active jobs to complete. `force = true` attempts a faster shutdown.
- **Events**:
  - `active`: Job started processing. `(job: Job) => void`
  - `completed`: Job finished successfully.
    `(job: Job, result: TResult) => void`
  - `failed`: Job failed (possibly after retries).
    `(job: Job | undefined, error: Error) => void`
  - `error`: An error occurred within the worker itself (e.g., Redis connection
    issue, lock renewal failure). `(error: Error, job?: Job) => void`
  - `retrying`: Job failed but will be retried.
    `(job: Job, error: Error) => void`
  - `movedDelayed`: Delayed jobs were moved to the wait list.
    `(count: number) => void`
  - `ready`: Worker connected to Redis. `() => void`
  - `closing`: Worker is starting the closing process. `() => void`
  - `closed`: Worker has finished closing. `() => void`

### `Job<TData, TResult, TName>`

- Represents a single job instance. Contains properties like:
  - `id`: Unique job ID.
  - `name`: Job name provided during `add`.
  - `data`: The payload provided during `add`.
  - `opts`: Job-specific options.
  - `attemptsMade`: How many times this job has been attempted.
  - `progress`: (Note: `updateProgress` is currently not fully implemented).
  - `returnValue`: The result returned by the processor upon completion.
  - `failedReason`: The error message if the job failed permanently.
  - `stacktrace`: Stack trace if the job failed.

## Configuration

### QueueOptions

- `connection`: IORedis connection options or an existing `IORedis` instance.
- `prefix`: Redis key prefix (default: `lightq`).
- `defaultJobOptions`: Default `JobOptions` applied to all jobs added to this
  queue.

### WorkerOptions

- Extends `QueueOptions`.
- `concurrency`: Max number of jobs to process concurrently (default: `1`).
- `lockDuration`: Time (ms) a job is locked during processing (default:
  `30000`).
- `lockRenewTime`: Interval (ms) before lock expiration to attempt renewal
  (default: `lockDuration / 2`).
- `removeOnComplete`: `true` to delete job data on success, or a `number` to
  keep only the latest N completed jobs' data (Note: Trimming by count requires
  uncommenting Lua script logic). Default: `false`.
- `removeOnFail`: `true` to delete job data on final failure, or a `number` to
  keep only the latest N failed jobs' data (Note: Trimming by count requires
  uncommenting Lua script logic). Default: `false`.

### JobOptions

- `jobId`: Assign a custom job ID.
- `delay`: Delay (ms) before the job should be processed.
- `attempts`: Max number of times to attempt the job (default: `1`).
- `backoff`: Backoff strategy for retries: `number` (fixed delay ms) or
  `{ type: 'fixed' | 'exponential', delay: number }`.
- `removeOnComplete`: Overrides worker/queue default.
- `removeOnFail`: Overrides worker/queue default.

## Contributing

Contributions are welcome! Please feel free to open an issue or submit a pull
request.

## License

Licensed under the [MIT License](LICENSE).
</file>

<file path="package.json">
{
  "name": "@jlucaso/lightq",
  "description": "LightQ is a simple and fast queue system for Node Bun and Deno, built with TypeScript and Redis. It provides a lightweight solution for managing background jobs and tasks in your applications.",
  "keywords": [
    "queue",
    "redis",
    "typescript",
    "nodejs",
    "background jobs",
    "task management",
    "bun",
    "deno"
  ],
  "version": "0.0.6",
  "private": false,
  "publishConfig": {
    "access": "public"
  },
  "files": [
    "dist"
  ],
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "scripts": {
    "build": "tsc && bun scripts/build.ts"
  },
  "author": {
    "email": "jlucaso@hotmail.com",
    "name": "Joo Lucas de Oliveira Lopes"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/jlucaso1/lightq.git"
  },
  "license": "MIT",
  "devDependencies": {
    "@types/bun": "latest",
    "@types/node": "^22.13.14",
    "luamin": "^1.0.4",
    "typescript": "^5"
  },
  "dependencies": {
    "croner": "^9.0.0",
    "ioredis": "^5.6.0"
  }
}
</file>

</files>
